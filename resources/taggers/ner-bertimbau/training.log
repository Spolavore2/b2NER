2025-06-29 02:35:27,298 ----------------------------------------------------------------------------------------------------
2025-06-29 02:35:27,299 Model: "SequenceTagger(
  (embeddings): TransformerWordEmbeddings(
    (model): BertModel(
      (embeddings): BertEmbeddings(
        (word_embeddings): Embedding(29795, 768, padding_idx=0)
        (position_embeddings): Embedding(512, 768)
        (token_type_embeddings): Embedding(2, 768)
        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (encoder): BertEncoder(
        (layer): ModuleList(
          (0-11): 12 x BertLayer(
            (attention): BertAttention(
              (self): BertSdpaSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
        )
      )
      (pooler): BertPooler(
        (dense): Linear(in_features=768, out_features=768, bias=True)
        (activation): Tanh()
      )
    )
  )
  (word_dropout): WordDropout(p=0.05)
  (locked_dropout): LockedDropout(p=0.5)
  (embedding2nn): Linear(in_features=768, out_features=768, bias=True)
  (linear): Linear(in_features=768, out_features=27, bias=True)
  (loss_function): ViterbiLoss()
  (crf): CRF()
)"
2025-06-29 02:35:27,299 ----------------------------------------------------------------------------------------------------
2025-06-29 02:35:27,299 Corpus: 239143 train + 29889 dev + 29886 test sentences
2025-06-29 02:35:27,300 ----------------------------------------------------------------------------------------------------
2025-06-29 02:35:27,300 Train:  239143 sentences
2025-06-29 02:35:27,300         (train_with_dev=False, train_with_test=False)
2025-06-29 02:35:27,300 ----------------------------------------------------------------------------------------------------
2025-06-29 02:35:27,300 Training Params:
2025-06-29 02:35:27,300  - learning_rate: "0.0002" 
2025-06-29 02:35:27,300  - mini_batch_size: "16"
2025-06-29 02:35:27,300  - max_epochs: "5"
2025-06-29 02:35:27,300  - shuffle: "True"
2025-06-29 02:35:27,300 ----------------------------------------------------------------------------------------------------
2025-06-29 02:35:27,300 Plugins:
2025-06-29 02:35:27,300  - AnnealOnPlateau | patience: '3', anneal_factor: '0.5', min_learning_rate: '0.0001'
2025-06-29 02:35:27,300 ----------------------------------------------------------------------------------------------------
2025-06-29 02:35:27,300 Final evaluation on model from best epoch (best-model.pt)
2025-06-29 02:35:27,300  - metric: "('micro avg', 'f1-score')"
2025-06-29 02:35:27,300 ----------------------------------------------------------------------------------------------------
2025-06-29 02:35:27,300 Computation:
2025-06-29 02:35:27,300  - compute on device: cuda:0
2025-06-29 02:35:27,300  - embedding storage: cpu
2025-06-29 02:35:27,300 ----------------------------------------------------------------------------------------------------
2025-06-29 02:35:27,300 Model training base path: "resources/taggers/ner-bertimbau"
2025-06-29 02:35:27,300 ----------------------------------------------------------------------------------------------------
2025-06-29 02:35:27,300 ----------------------------------------------------------------------------------------------------
2025-06-29 02:38:08,511 epoch 1 - iter 1494/14947 - loss 2.15838735 - time (sec): 161.21 - samples/sec: 1158.05 - lr: 0.000200 - momentum: 0.000000
2025-06-29 02:40:48,103 epoch 1 - iter 2988/14947 - loss 1.81066000 - time (sec): 320.80 - samples/sec: 1162.35 - lr: 0.000200 - momentum: 0.000000
2025-06-29 02:43:27,031 epoch 1 - iter 4482/14947 - loss 1.59024726 - time (sec): 479.73 - samples/sec: 1165.88 - lr: 0.000200 - momentum: 0.000000
2025-06-29 02:46:06,157 epoch 1 - iter 5976/14947 - loss 1.43496901 - time (sec): 638.86 - samples/sec: 1166.32 - lr: 0.000200 - momentum: 0.000000
2025-06-29 02:48:53,131 epoch 1 - iter 7470/14947 - loss 1.31419629 - time (sec): 805.83 - samples/sec: 1156.36 - lr: 0.000200 - momentum: 0.000000
2025-06-29 02:51:29,850 epoch 1 - iter 8964/14947 - loss 1.22017013 - time (sec): 962.55 - samples/sec: 1160.35 - lr: 0.000200 - momentum: 0.000000
2025-06-29 02:54:07,512 epoch 1 - iter 10458/14947 - loss 1.14406148 - time (sec): 1120.21 - samples/sec: 1161.65 - lr: 0.000200 - momentum: 0.000000
2025-06-29 02:56:45,372 epoch 1 - iter 11952/14947 - loss 1.07936085 - time (sec): 1278.07 - samples/sec: 1164.04 - lr: 0.000200 - momentum: 0.000000
2025-06-29 02:59:23,960 epoch 1 - iter 13446/14947 - loss 1.02267123 - time (sec): 1436.66 - samples/sec: 1164.32 - lr: 0.000200 - momentum: 0.000000
2025-06-29 03:02:00,762 epoch 1 - iter 14940/14947 - loss 0.97306577 - time (sec): 1593.46 - samples/sec: 1166.07 - lr: 0.000200 - momentum: 0.000000
2025-06-29 03:02:01,517 ----------------------------------------------------------------------------------------------------
2025-06-29 03:02:01,518 EPOCH 1 done: loss 0.9728 - lr: 0.000200
2025-06-29 03:03:53,511 DEV : loss 0.3242979347705841 - f1-score (micro avg)  0.8508
2025-06-29 03:03:53,844  - 0 epochs without improvement
2025-06-29 03:03:53,844 saving best model
2025-06-29 03:03:54,385 ----------------------------------------------------------------------------------------------------
2025-06-29 03:06:33,708 epoch 2 - iter 1494/14947 - loss 0.48198473 - time (sec): 159.32 - samples/sec: 1171.35 - lr: 0.000200 - momentum: 0.000000
2025-06-29 03:09:13,668 epoch 2 - iter 2988/14947 - loss 0.46863194 - time (sec): 319.28 - samples/sec: 1167.96 - lr: 0.000200 - momentum: 0.000000
2025-06-29 03:11:51,614 epoch 2 - iter 4482/14947 - loss 0.45393205 - time (sec): 477.23 - samples/sec: 1170.00 - lr: 0.000200 - momentum: 0.000000
2025-06-29 03:14:34,342 epoch 2 - iter 5976/14947 - loss 0.44029876 - time (sec): 639.96 - samples/sec: 1162.81 - lr: 0.000200 - momentum: 0.000000
2025-06-29 03:17:14,773 epoch 2 - iter 7470/14947 - loss 0.42917238 - time (sec): 800.39 - samples/sec: 1161.99 - lr: 0.000200 - momentum: 0.000000
2025-06-29 03:20:02,433 epoch 2 - iter 8964/14947 - loss 0.41726356 - time (sec): 968.05 - samples/sec: 1152.98 - lr: 0.000200 - momentum: 0.000000
2025-06-29 03:22:43,103 epoch 2 - iter 10458/14947 - loss 0.40858527 - time (sec): 1128.72 - samples/sec: 1153.13 - lr: 0.000200 - momentum: 0.000000
2025-06-29 03:25:22,903 epoch 2 - iter 11952/14947 - loss 0.40019664 - time (sec): 1288.52 - samples/sec: 1153.85 - lr: 0.000200 - momentum: 0.000000
2025-06-29 03:28:05,189 epoch 2 - iter 13446/14947 - loss 0.39204049 - time (sec): 1450.80 - samples/sec: 1152.93 - lr: 0.000200 - momentum: 0.000000
2025-06-29 03:30:46,356 epoch 2 - iter 14940/14947 - loss 0.38531547 - time (sec): 1611.97 - samples/sec: 1152.71 - lr: 0.000200 - momentum: 0.000000
2025-06-29 03:30:47,090 ----------------------------------------------------------------------------------------------------
2025-06-29 03:30:47,091 EPOCH 2 done: loss 0.3853 - lr: 0.000200
2025-06-29 03:31:34,861 DEV : loss 0.25951331853866577 - f1-score (micro avg)  0.8835
2025-06-29 03:31:35,200  - 0 epochs without improvement
2025-06-29 03:31:35,201 saving best model
2025-06-29 03:31:36,032 ----------------------------------------------------------------------------------------------------
2025-06-29 03:34:15,671 epoch 3 - iter 1494/14947 - loss 0.31291640 - time (sec): 159.64 - samples/sec: 1160.49 - lr: 0.000200 - momentum: 0.000000
2025-06-29 03:36:57,122 epoch 3 - iter 2988/14947 - loss 0.31116172 - time (sec): 321.09 - samples/sec: 1156.58 - lr: 0.000200 - momentum: 0.000000
2025-06-29 03:39:37,446 epoch 3 - iter 4482/14947 - loss 0.30913607 - time (sec): 481.41 - samples/sec: 1157.48 - lr: 0.000200 - momentum: 0.000000
2025-06-29 03:42:18,552 epoch 3 - iter 5976/14947 - loss 0.30788418 - time (sec): 642.52 - samples/sec: 1155.11 - lr: 0.000200 - momentum: 0.000000
2025-06-29 03:44:58,540 epoch 3 - iter 7470/14947 - loss 0.30656326 - time (sec): 802.51 - samples/sec: 1155.61 - lr: 0.000200 - momentum: 0.000000
2025-06-29 03:47:38,109 epoch 3 - iter 8964/14947 - loss 0.30344848 - time (sec): 962.08 - samples/sec: 1156.72 - lr: 0.000200 - momentum: 0.000000
2025-06-29 03:50:19,423 epoch 3 - iter 10458/14947 - loss 0.30091800 - time (sec): 1123.39 - samples/sec: 1156.47 - lr: 0.000200 - momentum: 0.000000
2025-06-29 03:52:59,901 epoch 3 - iter 11952/14947 - loss 0.29952669 - time (sec): 1283.87 - samples/sec: 1156.65 - lr: 0.000200 - momentum: 0.000000
2025-06-29 03:55:42,321 epoch 3 - iter 13446/14947 - loss 0.29861106 - time (sec): 1446.29 - samples/sec: 1156.03 - lr: 0.000200 - momentum: 0.000000
2025-06-29 03:58:23,649 epoch 3 - iter 14940/14947 - loss 0.29804264 - time (sec): 1607.62 - samples/sec: 1155.83 - lr: 0.000200 - momentum: 0.000000
2025-06-29 03:58:24,344 ----------------------------------------------------------------------------------------------------
2025-06-29 03:58:24,344 EPOCH 3 done: loss 0.2980 - lr: 0.000200
2025-06-29 03:59:13,104 DEV : loss 0.248943492770195 - f1-score (micro avg)  0.8821
2025-06-29 03:59:13,447  - 1 epochs without improvement
2025-06-29 03:59:13,447 ----------------------------------------------------------------------------------------------------
2025-06-29 04:01:52,951 epoch 4 - iter 1494/14947 - loss 0.28628281 - time (sec): 159.50 - samples/sec: 1153.49 - lr: 0.000200 - momentum: 0.000000
2025-06-29 04:04:33,819 epoch 4 - iter 2988/14947 - loss 0.27846910 - time (sec): 320.37 - samples/sec: 1152.96 - lr: 0.000200 - momentum: 0.000000
2025-06-29 04:07:13,039 epoch 4 - iter 4482/14947 - loss 0.28093479 - time (sec): 479.59 - samples/sec: 1157.59 - lr: 0.000200 - momentum: 0.000000
2025-06-29 04:09:52,766 epoch 4 - iter 5976/14947 - loss 0.27849997 - time (sec): 639.32 - samples/sec: 1160.66 - lr: 0.000200 - momentum: 0.000000
2025-06-29 04:12:36,133 epoch 4 - iter 7470/14947 - loss 0.27772674 - time (sec): 802.68 - samples/sec: 1155.49 - lr: 0.000200 - momentum: 0.000000
2025-06-29 04:15:17,120 epoch 4 - iter 8964/14947 - loss 0.27598011 - time (sec): 963.67 - samples/sec: 1154.71 - lr: 0.000200 - momentum: 0.000000
2025-06-29 04:17:57,959 epoch 4 - iter 10458/14947 - loss 0.27509587 - time (sec): 1124.51 - samples/sec: 1155.23 - lr: 0.000200 - momentum: 0.000000
2025-06-29 04:20:38,816 epoch 4 - iter 11952/14947 - loss 0.27342626 - time (sec): 1285.37 - samples/sec: 1155.37 - lr: 0.000200 - momentum: 0.000000
2025-06-29 04:23:19,758 epoch 4 - iter 13446/14947 - loss 0.27228363 - time (sec): 1446.31 - samples/sec: 1155.71 - lr: 0.000200 - momentum: 0.000000
2025-06-29 04:26:02,661 epoch 4 - iter 14940/14947 - loss 0.27108063 - time (sec): 1609.21 - samples/sec: 1154.69 - lr: 0.000200 - momentum: 0.000000
2025-06-29 04:26:03,353 ----------------------------------------------------------------------------------------------------
2025-06-29 04:26:03,353 EPOCH 4 done: loss 0.2711 - lr: 0.000200
2025-06-29 04:26:54,010 DEV : loss 0.24582606554031372 - f1-score (micro avg)  0.882
2025-06-29 04:26:54,355  - 2 epochs without improvement
2025-06-29 04:26:54,356 ----------------------------------------------------------------------------------------------------
2025-06-29 04:29:35,759 epoch 5 - iter 1494/14947 - loss 0.26302215 - time (sec): 161.40 - samples/sec: 1152.03 - lr: 0.000200 - momentum: 0.000000
2025-06-29 04:32:15,565 epoch 5 - iter 2988/14947 - loss 0.25697685 - time (sec): 321.21 - samples/sec: 1161.82 - lr: 0.000200 - momentum: 0.000000
2025-06-29 04:34:55,759 epoch 5 - iter 4482/14947 - loss 0.25900133 - time (sec): 481.40 - samples/sec: 1160.92 - lr: 0.000200 - momentum: 0.000000
2025-06-29 04:37:34,682 epoch 5 - iter 5976/14947 - loss 0.25977768 - time (sec): 640.33 - samples/sec: 1160.53 - lr: 0.000200 - momentum: 0.000000
2025-06-29 04:40:15,693 epoch 5 - iter 7470/14947 - loss 0.25837038 - time (sec): 801.34 - samples/sec: 1159.68 - lr: 0.000200 - momentum: 0.000000
2025-06-29 04:42:59,180 epoch 5 - iter 8964/14947 - loss 0.25803553 - time (sec): 964.82 - samples/sec: 1157.22 - lr: 0.000200 - momentum: 0.000000
2025-06-29 04:45:39,614 epoch 5 - iter 10458/14947 - loss 0.25585677 - time (sec): 1125.26 - samples/sec: 1157.20 - lr: 0.000200 - momentum: 0.000000
2025-06-29 04:48:27,798 epoch 5 - iter 11952/14947 - loss 0.25823124 - time (sec): 1293.44 - samples/sec: 1149.35 - lr: 0.000200 - momentum: 0.000000
2025-06-29 04:51:08,482 epoch 5 - iter 13446/14947 - loss 0.25697114 - time (sec): 1454.13 - samples/sec: 1150.33 - lr: 0.000200 - momentum: 0.000000
2025-06-29 04:53:49,093 epoch 5 - iter 14940/14947 - loss 0.25535985 - time (sec): 1614.74 - samples/sec: 1150.73 - lr: 0.000200 - momentum: 0.000000
2025-06-29 04:53:49,761 ----------------------------------------------------------------------------------------------------
2025-06-29 04:53:49,761 EPOCH 5 done: loss 0.2554 - lr: 0.000200
2025-06-29 04:54:31,970 DEV : loss 0.24405592679977417 - f1-score (micro avg)  0.8825
2025-06-29 04:54:32,308  - 3 epochs without improvement
2025-06-29 04:54:33,175 ----------------------------------------------------------------------------------------------------
2025-06-29 04:54:33,176 Loading model from best epoch ...
2025-06-29 04:54:35,346 SequenceTagger predicts: Dictionary with 27 tags: O, S-LOCALIZACAO, B-LOCALIZACAO, E-LOCALIZACAO, I-LOCALIZACAO, S-SETOR, B-SETOR, E-SETOR, I-SETOR, S-PORTE, B-PORTE, E-PORTE, I-PORTE, S-QTD_FUNCIONARIOS, B-QTD_FUNCIONARIOS, E-QTD_FUNCIONARIOS, I-QTD_FUNCIONARIOS, S-NOME_EMPRESA, B-NOME_EMPRESA, E-NOME_EMPRESA, I-NOME_EMPRESA, S-FATURAMENTO, B-FATURAMENTO, E-FATURAMENTO, I-FATURAMENTO, <START>, <STOP>
2025-06-29 04:56:10,533 
Results:
- F-score (micro) 0.9091
- F-score (macro) 0.9077
- Accuracy 0.8372

By class:
                  precision    recall  f1-score   support

     LOCALIZACAO     0.9376    0.9431    0.9404     12212
           SETOR     0.9566    0.8547    0.9028     10887
           PORTE     0.9874    0.9730    0.9801      9818
QTD_FUNCIONARIOS     0.9932    0.9977    0.9954      9513
    NOME_EMPRESA     0.6757    0.6355    0.6550      9304
     FATURAMENTO     0.9886    0.9567    0.9724      7429

       micro avg     0.9249    0.8939    0.9091     59163
       macro avg     0.9232    0.8934    0.9077     59163
    weighted avg     0.9235    0.8939    0.9080     59163

2025-06-29 04:56:10,533 ----------------------------------------------------------------------------------------------------
