Already up to date.
2025-06-23 16:28:14,871 Reading data from datasets/flair
2025-06-23 16:28:14,871 Train: datasets/flair/train.txt
2025-06-23 16:28:14,871 Dev: datasets/flair/dev.txt
2025-06-23 16:28:14,871 Test: datasets/flair/test.txt
tokenizer_config.json: 100% 43.0/43.0 [00:00<00:00, 319kB/s]
config.json: 100% 647/647 [00:00<00:00, 4.77MB/s]
vocab.txt: 100% 210k/210k [00:00<00:00, 4.25MB/s]
added_tokens.json: 100% 2.00/2.00 [00:00<00:00, 12.4kB/s]
special_tokens_map.json: 100% 112/112 [00:00<00:00, 1.13MB/s]
2025-06-23 16:29:00,554 Computing label dictionary. Progress:
0it [00:00, ?it/s]
179275it [00:04, 43925.96it/s]
2025-06-23 16:29:04,638 Dictionary created for label 'ner' with 5 values: LOCALIZACAO (seen 109005 times), NOME_EMPRESA (seen 105860 times), SETOR (seen 100632 times), PORTE (seen 88607 times), QTD_FUNCIONARIOS (seen 17265 times)
pytorch_model.bin: 100% 438M/438M [00:01<00:00, 223MB/s]
model.safetensors:   7% 31.5M/438M [00:00<00:02, 143MB/s] 2025-06-23 16:29:09,461 SequenceTagger predicts: Dictionary with 21 tags: O, S-LOCALIZACAO, B-LOCALIZACAO, E-LOCALIZACAO, I-LOCALIZACAO, S-NOME_EMPRESA, B-NOME_EMPRESA, E-NOME_EMPRESA, I-NOME_EMPRESA, S-SETOR, B-SETOR, E-SETOR, I-SETOR, S-PORTE, B-PORTE, E-PORTE, I-PORTE, S-QTD_FUNCIONARIOS, B-QTD_FUNCIONARIOS, E-QTD_FUNCIONARIOS, I-QTD_FUNCIONARIOS
model.safetensors:  14% 62.9M/438M [00:00<00:02, 180MB/s]/root/.cache/pypoetry/virtualenvs/b2ner-8-RvbF8z-py3.11/lib/python3.11/site-packages/flair/trainers/trainer.py:107: UserWarning: There should be no best model saved at epoch 1 except there is a model from previous trainings in your training folder. All previous best models will be deleted.
  warnings.warn(
/root/.cache/pypoetry/virtualenvs/b2ner-8-RvbF8z-py3.11/lib/python3.11/site-packages/flair/trainers/trainer.py:545: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=use_amp and flair.device.type != "cpu")
2025-06-23 16:29:09,559 ----------------------------------------------------------------------------------------------------
2025-06-23 16:29:09,560 Model: "SequenceTagger(
  (embeddings): TransformerWordEmbeddings(
    (model): BertModel(
      (embeddings): BertEmbeddings(
        (word_embeddings): Embedding(29795, 768, padding_idx=0)
        (position_embeddings): Embedding(512, 768)
        (token_type_embeddings): Embedding(2, 768)
        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (encoder): BertEncoder(
        (layer): ModuleList(
          (0-11): 12 x BertLayer(
            (attention): BertAttention(
              (self): BertSdpaSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
        )
      )
      (pooler): BertPooler(
        (dense): Linear(in_features=768, out_features=768, bias=True)
        (activation): Tanh()
      )
    )
  )
  (word_dropout): WordDropout(p=0.05)
  (locked_dropout): LockedDropout(p=0.5)
  (embedding2nn): Linear(in_features=768, out_features=768, bias=True)
  (rnn): LSTM(768, 128, batch_first=True, bidirectional=True)
  (linear): Linear(in_features=256, out_features=23, bias=True)
  (loss_function): ViterbiLoss()
  (crf): CRF()
)"
2025-06-23 16:29:09,560 ----------------------------------------------------------------------------------------------------
2025-06-23 16:29:09,560 Corpus: 179275 train + 22409 dev + 22406 test sentences
2025-06-23 16:29:09,561 ----------------------------------------------------------------------------------------------------
2025-06-23 16:29:09,561 Train:  179275 sentences
2025-06-23 16:29:09,561         (train_with_dev=False, train_with_test=False)
2025-06-23 16:29:09,561 ----------------------------------------------------------------------------------------------------
2025-06-23 16:29:09,561 Training Params:
2025-06-23 16:29:09,561  - learning_rate: "0.0002" 
2025-06-23 16:29:09,561  - mini_batch_size: "16"
2025-06-23 16:29:09,561  - max_epochs: "10"
2025-06-23 16:29:09,561  - shuffle: "True"
2025-06-23 16:29:09,561 ----------------------------------------------------------------------------------------------------
2025-06-23 16:29:09,561 Plugins:
2025-06-23 16:29:09,561  - AnnealOnPlateau | patience: '3', anneal_factor: '0.5', min_learning_rate: '0.0001'
2025-06-23 16:29:09,561 ----------------------------------------------------------------------------------------------------
2025-06-23 16:29:09,561 Final evaluation on model from best epoch (best-model.pt)
2025-06-23 16:29:09,561  - metric: "('micro avg', 'f1-score')"
2025-06-23 16:29:09,561 ----------------------------------------------------------------------------------------------------
2025-06-23 16:29:09,561 Computation:
2025-06-23 16:29:09,561  - compute on device: cuda:0
2025-06-23 16:29:09,561  - embedding storage: cpu
2025-06-23 16:29:09,561 ----------------------------------------------------------------------------------------------------
2025-06-23 16:29:09,561 Model training base path: "resources/taggers/ner-bertimbau"
2025-06-23 16:29:09,562 ----------------------------------------------------------------------------------------------------
2025-06-23 16:29:09,562 ----------------------------------------------------------------------------------------------------
model.safetensors: 100% 438M/438M [00:02<00:00, 209MB/s]
2025-06-23 16:31:13,250 epoch 1 - iter 1120/11205 - loss 3.32808516 - time (sec): 123.69 - samples/sec: 1082.57 - lr: 0.000200 - momentum: 0.000000
2025-06-23 16:33:21,094 epoch 1 - iter 2240/11205 - loss 3.05347480 - time (sec): 251.53 - samples/sec: 1067.08 - lr: 0.000200 - momentum: 0.000000
2025-06-23 16:35:27,792 epoch 1 - iter 3360/11205 - loss 2.87299820 - time (sec): 378.23 - samples/sec: 1067.98 - lr: 0.000200 - momentum: 0.000000
2025-06-23 16:37:35,656 epoch 1 - iter 4480/11205 - loss 2.74373885 - time (sec): 506.09 - samples/sec: 1064.48 - lr: 0.000200 - momentum: 0.000000
2025-06-23 16:39:42,776 epoch 1 - iter 5600/11205 - loss 2.64147357 - time (sec): 633.21 - samples/sec: 1064.73 - lr: 0.000200 - momentum: 0.000000
2025-06-23 16:41:49,679 epoch 1 - iter 6720/11205 - loss 2.56015817 - time (sec): 760.12 - samples/sec: 1063.62 - lr: 0.000200 - momentum: 0.000000
2025-06-23 16:43:55,395 epoch 1 - iter 7840/11205 - loss 2.48966217 - time (sec): 885.83 - samples/sec: 1064.63 - lr: 0.000200 - momentum: 0.000000
2025-06-23 16:46:00,540 epoch 1 - iter 8960/11205 - loss 2.42528033 - time (sec): 1010.98 - samples/sec: 1066.14 - lr: 0.000200 - momentum: 0.000000
2025-06-23 16:48:07,289 epoch 1 - iter 10080/11205 - loss 2.36520586 - time (sec): 1137.73 - samples/sec: 1066.06 - lr: 0.000200 - momentum: 0.000000
2025-06-23 16:50:12,276 epoch 1 - iter 11200/11205 - loss 2.30908558 - time (sec): 1262.71 - samples/sec: 1066.86 - lr: 0.000200 - momentum: 0.000000
2025-06-23 16:50:12,763 ----------------------------------------------------------------------------------------------------
2025-06-23 16:50:12,763 EPOCH 1 done: loss 2.3089 - lr: 0.000200
100% 351/351 [01:31<00:00,  3.82it/s]
2025-06-23 16:51:45,098 DEV : loss 1.5624600648880005 - f1-score (micro avg)  0.2661
2025-06-23 16:51:45,359  - 0 epochs without improvement
2025-06-23 16:51:45,359 saving best model
2025-06-23 16:51:45,960 ----------------------------------------------------------------------------------------------------
2025-06-23 16:53:51,066 epoch 2 - iter 1120/11205 - loss 1.71459814 - time (sec): 125.10 - samples/sec: 1078.52 - lr: 0.000200 - momentum: 0.000000
2025-06-23 16:55:58,260 epoch 2 - iter 2240/11205 - loss 1.67606114 - time (sec): 252.30 - samples/sec: 1070.79 - lr: 0.000200 - momentum: 0.000000
2025-06-23 16:58:04,049 epoch 2 - iter 3360/11205 - loss 1.63840940 - time (sec): 378.09 - samples/sec: 1071.32 - lr: 0.000200 - momentum: 0.000000
2025-06-23 17:00:09,266 epoch 2 - iter 4480/11205 - loss 1.60333855 - time (sec): 503.30 - samples/sec: 1073.57 - lr: 0.000200 - momentum: 0.000000
2025-06-23 17:02:17,167 epoch 2 - iter 5600/11205 - loss 1.57212981 - time (sec): 631.21 - samples/sec: 1071.31 - lr: 0.000200 - momentum: 0.000000
2025-06-23 17:04:25,099 epoch 2 - iter 6720/11205 - loss 1.54367831 - time (sec): 759.14 - samples/sec: 1066.90 - lr: 0.000200 - momentum: 0.000000
2025-06-23 17:06:36,989 epoch 2 - iter 7840/11205 - loss 1.51452712 - time (sec): 891.03 - samples/sec: 1061.20 - lr: 0.000200 - momentum: 0.000000
2025-06-23 17:08:44,983 epoch 2 - iter 8960/11205 - loss 1.48784518 - time (sec): 1019.02 - samples/sec: 1059.41 - lr: 0.000200 - momentum: 0.000000
2025-06-23 17:10:52,125 epoch 2 - iter 10080/11205 - loss 1.46359560 - time (sec): 1146.16 - samples/sec: 1058.23 - lr: 0.000200 - momentum: 0.000000
2025-06-23 17:12:58,851 epoch 2 - iter 11200/11205 - loss 1.43839498 - time (sec): 1272.89 - samples/sec: 1058.29 - lr: 0.000200 - momentum: 0.000000
2025-06-23 17:12:59,429 ----------------------------------------------------------------------------------------------------
2025-06-23 17:12:59,429 EPOCH 2 done: loss 1.4383 - lr: 0.000200
100% 351/351 [00:39<00:00,  8.91it/s]
2025-06-23 17:13:39,262 DEV : loss 1.1728450059890747 - f1-score (micro avg)  0.5374
2025-06-23 17:13:39,527  - 0 epochs without improvement
2025-06-23 17:13:39,528 saving best model
2025-06-23 17:13:40,466 ----------------------------------------------------------------------------------------------------
2025-06-23 17:15:49,001 epoch 3 - iter 1120/11205 - loss 1.17646380 - time (sec): 128.53 - samples/sec: 1045.77 - lr: 0.000200 - momentum: 0.000000
2025-06-23 17:17:58,062 epoch 3 - iter 2240/11205 - loss 1.15404627 - time (sec): 257.59 - samples/sec: 1046.94 - lr: 0.000200 - momentum: 0.000000
2025-06-23 17:20:05,957 epoch 3 - iter 3360/11205 - loss 1.13670486 - time (sec): 385.49 - samples/sec: 1050.62 - lr: 0.000200 - momentum: 0.000000
2025-06-23 17:22:14,876 epoch 3 - iter 4480/11205 - loss 1.11991830 - time (sec): 514.41 - samples/sec: 1049.33 - lr: 0.000200 - momentum: 0.000000
2025-06-23 17:24:22,402 epoch 3 - iter 5600/11205 - loss 1.10313397 - time (sec): 641.93 - samples/sec: 1048.64 - lr: 0.000200 - momentum: 0.000000
2025-06-23 17:26:30,368 epoch 3 - iter 6720/11205 - loss 1.08838751 - time (sec): 769.90 - samples/sec: 1049.60 - lr: 0.000200 - momentum: 0.000000
2025-06-23 17:28:37,838 epoch 3 - iter 7840/11205 - loss 1.07383422 - time (sec): 897.37 - samples/sec: 1049.19 - lr: 0.000200 - momentum: 0.000000
2025-06-23 17:30:45,524 epoch 3 - iter 8960/11205 - loss 1.05751738 - time (sec): 1025.06 - samples/sec: 1049.48 - lr: 0.000200 - momentum: 0.000000
2025-06-23 17:32:53,172 epoch 3 - iter 10080/11205 - loss 1.04064615 - time (sec): 1152.70 - samples/sec: 1051.61 - lr: 0.000200 - momentum: 0.000000
2025-06-23 17:35:00,830 epoch 3 - iter 11200/11205 - loss 1.02611109 - time (sec): 1280.36 - samples/sec: 1052.12 - lr: 0.000200 - momentum: 0.000000
2025-06-23 17:35:01,410 ----------------------------------------------------------------------------------------------------
2025-06-23 17:35:01,410 EPOCH 3 done: loss 1.0260 - lr: 0.000200
100% 351/351 [00:40<00:00,  8.69it/s]
2025-06-23 17:35:42,235 DEV : loss 1.0367379188537598 - f1-score (micro avg)  0.6422
2025-06-23 17:35:42,496  - 0 epochs without improvement
2025-06-23 17:35:42,497 saving best model
2025-06-23 17:35:43,323 ----------------------------------------------------------------------------------------------------
2025-06-23 17:37:51,078 epoch 4 - iter 1120/11205 - loss 0.86838060 - time (sec): 127.75 - samples/sec: 1055.06 - lr: 0.000200 - momentum: 0.000000
2025-06-23 17:39:58,942 epoch 4 - iter 2240/11205 - loss 0.85278607 - time (sec): 255.62 - samples/sec: 1053.90 - lr: 0.000200 - momentum: 0.000000
2025-06-23 17:42:06,214 epoch 4 - iter 3360/11205 - loss 0.84162847 - time (sec): 382.89 - samples/sec: 1056.02 - lr: 0.000200 - momentum: 0.000000
2025-06-23 17:44:13,764 epoch 4 - iter 4480/11205 - loss 0.83275988 - time (sec): 510.44 - samples/sec: 1055.59 - lr: 0.000200 - momentum: 0.000000
2025-06-23 17:46:21,540 epoch 4 - iter 5600/11205 - loss 0.82450054 - time (sec): 638.22 - samples/sec: 1055.48 - lr: 0.000200 - momentum: 0.000000
2025-06-23 17:48:27,463 epoch 4 - iter 6720/11205 - loss 0.81470681 - time (sec): 764.14 - samples/sec: 1057.21 - lr: 0.000200 - momentum: 0.000000
2025-06-23 17:50:31,751 epoch 4 - iter 7840/11205 - loss 0.80385807 - time (sec): 888.43 - samples/sec: 1060.54 - lr: 0.000200 - momentum: 0.000000
2025-06-23 17:52:37,119 epoch 4 - iter 8960/11205 - loss 0.79317132 - time (sec): 1013.79 - samples/sec: 1062.61 - lr: 0.000200 - momentum: 0.000000
2025-06-23 17:54:44,413 epoch 4 - iter 10080/11205 - loss 0.78321011 - time (sec): 1141.09 - samples/sec: 1062.00 - lr: 0.000200 - momentum: 0.000000
2025-06-23 17:56:54,746 epoch 4 - iter 11200/11205 - loss 0.77501250 - time (sec): 1271.42 - samples/sec: 1059.52 - lr: 0.000200 - momentum: 0.000000
2025-06-23 17:56:55,282 ----------------------------------------------------------------------------------------------------
2025-06-23 17:56:55,282 EPOCH 4 done: loss 0.7749 - lr: 0.000200
100% 351/351 [00:38<00:00,  9.15it/s]
2025-06-23 17:57:34,076 DEV : loss 0.9746909737586975 - f1-score (micro avg)  0.7043
2025-06-23 17:57:34,338  - 0 epochs without improvement
2025-06-23 17:57:34,338 saving best model
2025-06-23 17:57:35,224 ----------------------------------------------------------------------------------------------------
2025-06-23 17:59:41,185 epoch 5 - iter 1120/11205 - loss 0.68068165 - time (sec): 125.96 - samples/sec: 1065.53 - lr: 0.000200 - momentum: 0.000000
2025-06-23 18:01:47,830 epoch 5 - iter 2240/11205 - loss 0.67096558 - time (sec): 252.60 - samples/sec: 1062.82 - lr: 0.000200 - momentum: 0.000000
2025-06-23 18:03:54,820 epoch 5 - iter 3360/11205 - loss 0.66301249 - time (sec): 379.59 - samples/sec: 1061.85 - lr: 0.000200 - momentum: 0.000000
2025-06-23 18:06:02,783 epoch 5 - iter 4480/11205 - loss 0.65748681 - time (sec): 507.56 - samples/sec: 1060.05 - lr: 0.000200 - momentum: 0.000000
2025-06-23 18:08:09,286 epoch 5 - iter 5600/11205 - loss 0.65146268 - time (sec): 634.06 - samples/sec: 1060.51 - lr: 0.000200 - momentum: 0.000000
2025-06-23 18:10:16,361 epoch 5 - iter 6720/11205 - loss 0.64446332 - time (sec): 761.13 - samples/sec: 1059.41 - lr: 0.000200 - momentum: 0.000000
2025-06-23 18:12:25,630 epoch 5 - iter 7840/11205 - loss 0.63874816 - time (sec): 890.40 - samples/sec: 1058.47 - lr: 0.000200 - momentum: 0.000000
2025-06-23 18:14:31,880 epoch 5 - iter 8960/11205 - loss 0.63223248 - time (sec): 1016.65 - samples/sec: 1060.35 - lr: 0.000200 - momentum: 0.000000
2025-06-23 18:16:36,744 epoch 5 - iter 10080/11205 - loss 0.62833246 - time (sec): 1141.52 - samples/sec: 1061.44 - lr: 0.000200 - momentum: 0.000000
2025-06-23 18:18:45,015 epoch 5 - iter 11200/11205 - loss 0.62183565 - time (sec): 1269.79 - samples/sec: 1060.88 - lr: 0.000200 - momentum: 0.000000
2025-06-23 18:18:45,543 ----------------------------------------------------------------------------------------------------
2025-06-23 18:18:45,544 EPOCH 5 done: loss 0.6218 - lr: 0.000200
100% 351/351 [00:39<00:00,  8.92it/s]
2025-06-23 18:19:25,291 DEV : loss 0.9430956840515137 - f1-score (micro avg)  0.7429
2025-06-23 18:19:25,545  - 0 epochs without improvement
2025-06-23 18:19:25,546 saving best model
2025-06-23 18:19:26,380 ----------------------------------------------------------------------------------------------------
2025-06-23 18:21:32,592 epoch 6 - iter 1120/11205 - loss 0.56459002 - time (sec): 126.21 - samples/sec: 1070.78 - lr: 0.000200 - momentum: 0.000000
2025-06-23 18:23:39,966 epoch 6 - iter 2240/11205 - loss 0.56045771 - time (sec): 253.58 - samples/sec: 1066.46 - lr: 0.000200 - momentum: 0.000000
2025-06-23 18:25:47,298 epoch 6 - iter 3360/11205 - loss 0.55550190 - time (sec): 380.92 - samples/sec: 1063.78 - lr: 0.000200 - momentum: 0.000000
2025-06-23 18:27:54,361 epoch 6 - iter 4480/11205 - loss 0.55311772 - time (sec): 507.98 - samples/sec: 1064.10 - lr: 0.000200 - momentum: 0.000000
2025-06-23 18:29:59,887 epoch 6 - iter 5600/11205 - loss 0.54814048 - time (sec): 633.51 - samples/sec: 1064.75 - lr: 0.000200 - momentum: 0.000000
2025-06-23 18:32:05,608 epoch 6 - iter 6720/11205 - loss 0.54615217 - time (sec): 759.23 - samples/sec: 1064.99 - lr: 0.000200 - momentum: 0.000000
2025-06-23 18:34:11,823 epoch 6 - iter 7840/11205 - loss 0.54121377 - time (sec): 885.44 - samples/sec: 1065.79 - lr: 0.000200 - momentum: 0.000000
2025-06-23 18:36:13,545 epoch 6 - iter 8960/11205 - loss 0.53723797 - time (sec): 1007.16 - samples/sec: 1070.51 - lr: 0.000200 - momentum: 0.000000
2025-06-23 18:38:18,005 epoch 6 - iter 10080/11205 - loss 0.53432050 - time (sec): 1131.62 - samples/sec: 1071.27 - lr: 0.000200 - momentum: 0.000000
2025-06-23 18:40:24,416 epoch 6 - iter 11200/11205 - loss 0.53023439 - time (sec): 1258.04 - samples/sec: 1070.79 - lr: 0.000200 - momentum: 0.000000
2025-06-23 18:40:24,942 ----------------------------------------------------------------------------------------------------
2025-06-23 18:40:24,942 EPOCH 6 done: loss 0.5302 - lr: 0.000200
100% 351/351 [00:37<00:00,  9.47it/s]
2025-06-23 18:41:02,413 DEV : loss 0.9267679452896118 - f1-score (micro avg)  0.759
2025-06-23 18:41:02,670  - 0 epochs without improvement
2025-06-23 18:41:02,670 saving best model
2025-06-23 18:41:03,557 ----------------------------------------------------------------------------------------------------
2025-06-23 18:43:07,971 epoch 7 - iter 1120/11205 - loss 0.49564553 - time (sec): 124.41 - samples/sec: 1085.35 - lr: 0.000200 - momentum: 0.000000
2025-06-23 18:45:10,962 epoch 7 - iter 2240/11205 - loss 0.49137817 - time (sec): 247.40 - samples/sec: 1088.32 - lr: 0.000200 - momentum: 0.000000
2025-06-23 18:47:14,657 epoch 7 - iter 3360/11205 - loss 0.48626696 - time (sec): 371.10 - samples/sec: 1089.59 - lr: 0.000200 - momentum: 0.000000
2025-06-23 18:49:19,521 epoch 7 - iter 4480/11205 - loss 0.48663771 - time (sec): 495.96 - samples/sec: 1086.21 - lr: 0.000200 - momentum: 0.000000
2025-06-23 18:51:26,189 epoch 7 - iter 5600/11205 - loss 0.48414515 - time (sec): 622.63 - samples/sec: 1083.73 - lr: 0.000200 - momentum: 0.000000
2025-06-23 18:53:30,205 epoch 7 - iter 6720/11205 - loss 0.48204464 - time (sec): 746.65 - samples/sec: 1083.94 - lr: 0.000200 - momentum: 0.000000
2025-06-23 18:55:33,807 epoch 7 - iter 7840/11205 - loss 0.47947134 - time (sec): 870.25 - samples/sec: 1085.21 - lr: 0.000200 - momentum: 0.000000
2025-06-23 18:57:37,459 epoch 7 - iter 8960/11205 - loss 0.47759018 - time (sec): 993.90 - samples/sec: 1085.62 - lr: 0.000200 - momentum: 0.000000
2025-06-23 18:59:41,745 epoch 7 - iter 10080/11205 - loss 0.47509052 - time (sec): 1118.19 - samples/sec: 1085.24 - lr: 0.000200 - momentum: 0.000000
2025-06-23 19:01:45,596 epoch 7 - iter 11200/11205 - loss 0.47325420 - time (sec): 1242.04 - samples/sec: 1084.52 - lr: 0.000200 - momentum: 0.000000
2025-06-23 19:01:46,116 ----------------------------------------------------------------------------------------------------
2025-06-23 19:01:46,116 EPOCH 7 done: loss 0.4732 - lr: 0.000200
100% 351/351 [00:37<00:00,  9.33it/s]
2025-06-23 19:02:24,092 DEV : loss 0.9187129735946655 - f1-score (micro avg)  0.7666
2025-06-23 19:02:24,345  - 0 epochs without improvement
2025-06-23 19:02:24,345 saving best model
2025-06-23 19:02:25,233 ----------------------------------------------------------------------------------------------------
2025-06-23 19:04:28,522 epoch 8 - iter 1120/11205 - loss 0.45556226 - time (sec): 123.29 - samples/sec: 1084.78 - lr: 0.000200 - momentum: 0.000000
2025-06-23 19:06:31,029 epoch 8 - iter 2240/11205 - loss 0.44730921 - time (sec): 245.80 - samples/sec: 1090.67 - lr: 0.000200 - momentum: 0.000000
2025-06-23 19:08:37,426 epoch 8 - iter 3360/11205 - loss 0.44435362 - time (sec): 372.19 - samples/sec: 1084.17 - lr: 0.000200 - momentum: 0.000000
2025-06-23 19:10:44,072 epoch 8 - iter 4480/11205 - loss 0.44320217 - time (sec): 498.84 - samples/sec: 1076.95 - lr: 0.000200 - momentum: 0.000000
2025-06-23 19:12:48,602 epoch 8 - iter 5600/11205 - loss 0.44303745 - time (sec): 623.37 - samples/sec: 1078.74 - lr: 0.000200 - momentum: 0.000000
2025-06-23 19:14:51,697 epoch 8 - iter 6720/11205 - loss 0.44163458 - time (sec): 746.46 - samples/sec: 1081.65 - lr: 0.000200 - momentum: 0.000000
2025-06-23 19:16:55,202 epoch 8 - iter 7840/11205 - loss 0.44088004 - time (sec): 869.97 - samples/sec: 1082.05 - lr: 0.000200 - momentum: 0.000000
2025-06-23 19:18:58,582 epoch 8 - iter 8960/11205 - loss 0.43824182 - time (sec): 993.35 - samples/sec: 1083.02 - lr: 0.000200 - momentum: 0.000000
2025-06-23 19:21:05,656 epoch 8 - iter 10080/11205 - loss 0.43644264 - time (sec): 1120.42 - samples/sec: 1081.22 - lr: 0.000200 - momentum: 0.000000
2025-06-23 19:23:10,367 epoch 8 - iter 11200/11205 - loss 0.43411427 - time (sec): 1245.13 - samples/sec: 1081.92 - lr: 0.000200 - momentum: 0.000000
2025-06-23 19:23:10,843 ----------------------------------------------------------------------------------------------------
2025-06-23 19:23:10,844 EPOCH 8 done: loss 0.4341 - lr: 0.000200
100% 351/351 [00:37<00:00,  9.43it/s]
2025-06-23 19:23:48,425 DEV : loss 0.9126415252685547 - f1-score (micro avg)  0.7711
2025-06-23 19:23:48,679  - 0 epochs without improvement
2025-06-23 19:23:48,679 saving best model
2025-06-23 19:23:49,582 ----------------------------------------------------------------------------------------------------
2025-06-23 19:25:52,584 epoch 9 - iter 1120/11205 - loss 0.41623152 - time (sec): 123.00 - samples/sec: 1097.18 - lr: 0.000200 - momentum: 0.000000
2025-06-23 19:27:55,447 epoch 9 - iter 2240/11205 - loss 0.41582565 - time (sec): 245.86 - samples/sec: 1092.80 - lr: 0.000200 - momentum: 0.000000
2025-06-23 19:29:59,828 epoch 9 - iter 3360/11205 - loss 0.41366257 - time (sec): 370.24 - samples/sec: 1091.02 - lr: 0.000200 - momentum: 0.000000
2025-06-23 19:32:04,089 epoch 9 - iter 4480/11205 - loss 0.41088250 - time (sec): 494.51 - samples/sec: 1088.05 - lr: 0.000200 - momentum: 0.000000
2025-06-23 19:34:10,499 epoch 9 - iter 5600/11205 - loss 0.41260789 - time (sec): 620.92 - samples/sec: 1083.52 - lr: 0.000200 - momentum: 0.000000
2025-06-23 19:36:13,481 epoch 9 - iter 6720/11205 - loss 0.41041818 - time (sec): 743.90 - samples/sec: 1085.15 - lr: 0.000200 - momentum: 0.000000
2025-06-23 19:38:25,385 epoch 9 - iter 7840/11205 - loss 0.40870331 - time (sec): 875.80 - samples/sec: 1075.49 - lr: 0.000200 - momentum: 0.000000
2025-06-23 19:40:29,333 epoch 9 - iter 8960/11205 - loss 0.40749684 - time (sec): 999.75 - samples/sec: 1077.14 - lr: 0.000200 - momentum: 0.000000
2025-06-23 19:42:36,814 epoch 9 - iter 10080/11205 - loss 0.40727879 - time (sec): 1127.23 - samples/sec: 1075.47 - lr: 0.000200 - momentum: 0.000000
2025-06-23 19:44:40,111 epoch 9 - iter 11200/11205 - loss 0.40722662 - time (sec): 1250.53 - samples/sec: 1077.19 - lr: 0.000200 - momentum: 0.000000
2025-06-23 19:44:40,644 ----------------------------------------------------------------------------------------------------
2025-06-23 19:44:40,644 EPOCH 9 done: loss 0.4073 - lr: 0.000200
100% 351/351 [00:32<00:00, 10.81it/s]
2025-06-23 19:45:13,570 DEV : loss 0.9077733755111694 - f1-score (micro avg)  0.7751
2025-06-23 19:45:13,833  - 0 epochs without improvement
2025-06-23 19:45:13,834 saving best model
2025-06-23 19:45:14,776 ----------------------------------------------------------------------------------------------------
2025-06-23 19:47:22,589 epoch 10 - iter 1120/11205 - loss 0.40523545 - time (sec): 127.81 - samples/sec: 1058.33 - lr: 0.000200 - momentum: 0.000000
2025-06-23 19:49:28,495 epoch 10 - iter 2240/11205 - loss 0.39307425 - time (sec): 253.72 - samples/sec: 1064.92 - lr: 0.000200 - momentum: 0.000000
2025-06-23 19:51:36,065 epoch 10 - iter 3360/11205 - loss 0.39188282 - time (sec): 381.29 - samples/sec: 1064.68 - lr: 0.000200 - momentum: 0.000000
2025-06-23 19:53:43,984 epoch 10 - iter 4480/11205 - loss 0.38794749 - time (sec): 509.21 - samples/sec: 1063.72 - lr: 0.000200 - momentum: 0.000000
2025-06-23 19:55:53,501 epoch 10 - iter 5600/11205 - loss 0.38991935 - time (sec): 638.72 - samples/sec: 1058.20 - lr: 0.000200 - momentum: 0.000000
2025-06-23 19:58:01,693 epoch 10 - iter 6720/11205 - loss 0.38905975 - time (sec): 766.92 - samples/sec: 1055.68 - lr: 0.000200 - momentum: 0.000000
2025-06-23 20:00:05,549 epoch 10 - iter 7840/11205 - loss 0.38671758 - time (sec): 890.77 - samples/sec: 1059.98 - lr: 0.000200 - momentum: 0.000000
2025-06-23 20:02:15,055 epoch 10 - iter 8960/11205 - loss 0.38600087 - time (sec): 1020.28 - samples/sec: 1056.72 - lr: 0.000200 - momentum: 0.000000
2025-06-23 20:04:21,044 epoch 10 - iter 10080/11205 - loss 0.38563331 - time (sec): 1146.27 - samples/sec: 1057.79 - lr: 0.000200 - momentum: 0.000000
2025-06-23 20:06:25,679 epoch 10 - iter 11200/11205 - loss 0.38473783 - time (sec): 1270.90 - samples/sec: 1059.99 - lr: 0.000200 - momentum: 0.000000
2025-06-23 20:06:26,183 ----------------------------------------------------------------------------------------------------
2025-06-23 20:06:26,183 EPOCH 10 done: loss 0.3848 - lr: 0.000200
100% 351/351 [00:38<00:00,  9.18it/s]
2025-06-23 20:07:04,791 DEV : loss 0.903412938117981 - f1-score (micro avg)  0.7771
2025-06-23 20:07:05,060  - 0 epochs without improvement
2025-06-23 20:07:05,061 saving best model
2025-06-23 20:07:06,669 ----------------------------------------------------------------------------------------------------
2025-06-23 20:07:06,670 Loading model from best epoch ...
2025-06-23 20:07:08,955 SequenceTagger predicts: Dictionary with 23 tags: O, S-LOCALIZACAO, B-LOCALIZACAO, E-LOCALIZACAO, I-LOCALIZACAO, S-NOME_EMPRESA, B-NOME_EMPRESA, E-NOME_EMPRESA, I-NOME_EMPRESA, S-SETOR, B-SETOR, E-SETOR, I-SETOR, S-PORTE, B-PORTE, E-PORTE, I-PORTE, S-QTD_FUNCIONARIOS, B-QTD_FUNCIONARIOS, E-QTD_FUNCIONARIOS, I-QTD_FUNCIONARIOS, <START>, <STOP>
100% 351/351 [01:13<00:00,  4.78it/s]
2025-06-23 20:08:22,978 
Results:
- F-score (micro) 0.8885
- F-score (macro) 0.908
- Accuracy 0.8035

By class:
                  precision    recall  f1-score   support

     LOCALIZACAO     0.9581    0.9681    0.9631     13655
    NOME_EMPRESA     0.7564    0.7930    0.7743     13149
           SETOR     0.8227    0.8316    0.8271     12506
           PORTE     0.9968    0.9723    0.9844     11163
QTD_FUNCIONARIOS     0.9839    0.9986    0.9912      2200

       micro avg     0.8829    0.8941    0.8885     52673
       macro avg     0.9036    0.9127    0.9080     52673
    weighted avg     0.8849    0.8941    0.8894     52673

2025-06-23 20:08:22,978 ----------------------------------------------------------------------------------------------------

