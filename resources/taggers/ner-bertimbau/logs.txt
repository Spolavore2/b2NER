Already up to date.
2025-06-27 00:11:58,757 Reading data from datasets/flair
2025-06-27 00:11:58,757 Train: datasets/flair/train.txt
2025-06-27 00:11:58,757 Dev: datasets/flair/dev.txt
2025-06-27 00:11:58,757 Test: datasets/flair/test.txt
tokenizer_config.json: 100% 43.0/43.0 [00:00<00:00, 256kB/s]
config.json: 100% 647/647 [00:00<00:00, 5.01MB/s]
vocab.txt: 210kB [00:00, 46.0MB/s]
added_tokens.json: 100% 2.00/2.00 [00:00<00:00, 9.03kB/s]
special_tokens_map.json: 100% 112/112 [00:00<00:00, 1.03MB/s]
2025-06-27 00:12:59,359 Computing label dictionary. Progress:
0it [00:00, ?it/s]
224098it [00:04, 45656.90it/s]
2025-06-27 00:13:04,270 Dictionary created for label 'ner' with 6 values: LOCALIZACAO (seen 131862 times), SETOR (seen 124735 times), PORTE (seen 111101 times), NOME_EMPRESA (seen 105896 times), QTD_FUNCIONARIOS (seen 42859 times), FATURAMENTO (seen 31303 times)
pytorch_model.bin: 100% 438M/438M [00:00<00:00, 541MB/s]
2025-06-27 00:13:08,849 SequenceTagger predicts: Dictionary with 25 tags: O, S-LOCALIZACAO, B-LOCALIZACAO, E-LOCALIZACAO, I-LOCALIZACAO, S-SETOR, B-SETOR, E-SETOR, I-SETOR, S-PORTE, B-PORTE, E-PORTE, I-PORTE, S-NOME_EMPRESA, B-NOME_EMPRESA, E-NOME_EMPRESA, I-NOME_EMPRESA, S-QTD_FUNCIONARIOS, B-QTD_FUNCIONARIOS, E-QTD_FUNCIONARIOS, I-QTD_FUNCIONARIOS, S-FATURAMENTO, B-FATURAMENTO, E-FATURAMENTO, I-FATURAMENTO
/root/.cache/pypoetry/virtualenvs/b2ner-8-RvbF8z-py3.11/lib/python3.11/site-packages/flair/trainers/trainer.py:107: UserWarning: There should be no best model saved at epoch 1 except there is a model from previous trainings in your training folder. All previous best models will be deleted.
  warnings.warn(
/root/.cache/pypoetry/virtualenvs/b2ner-8-RvbF8z-py3.11/lib/python3.11/site-packages/flair/trainers/trainer.py:545: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=use_amp and flair.device.type != "cpu")
2025-06-27 00:13:08,921 ----------------------------------------------------------------------------------------------------
2025-06-27 00:13:08,922 Model: "SequenceTagger(
  (embeddings): TransformerWordEmbeddings(
    (model): BertModel(
      (embeddings): BertEmbeddings(
        (word_embeddings): Embedding(29795, 768, padding_idx=0)
        (position_embeddings): Embedding(512, 768)
        (token_type_embeddings): Embedding(2, 768)
        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (encoder): BertEncoder(
        (layer): ModuleList(
          (0-11): 12 x BertLayer(
            (attention): BertAttention(
              (self): BertSdpaSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
        )
      )
      (pooler): BertPooler(
        (dense): Linear(in_features=768, out_features=768, bias=True)
        (activation): Tanh()
      )
    )
  )
  (word_dropout): WordDropout(p=0.05)
  (locked_dropout): LockedDropout(p=0.5)
  (embedding2nn): Linear(in_features=768, out_features=768, bias=True)
  (rnn): LSTM(768, 128, batch_first=True, bidirectional=True)
  (linear): Linear(in_features=256, out_features=27, bias=True)
  (loss_function): ViterbiLoss()
  (crf): CRF()
)"
2025-06-27 00:13:08,922 ----------------------------------------------------------------------------------------------------
2025-06-27 00:13:08,923 Corpus: 224098 train + 28012 dev + 28013 test sentences
2025-06-27 00:13:08,923 ----------------------------------------------------------------------------------------------------
2025-06-27 00:13:08,923 Train:  224098 sentences
2025-06-27 00:13:08,923         (train_with_dev=False, train_with_test=False)
2025-06-27 00:13:08,923 ----------------------------------------------------------------------------------------------------
2025-06-27 00:13:08,923 Training Params:
2025-06-27 00:13:08,923  - learning_rate: "0.0002" 
2025-06-27 00:13:08,923  - mini_batch_size: "16"
2025-06-27 00:13:08,923  - max_epochs: "10"
2025-06-27 00:13:08,923  - shuffle: "True"
2025-06-27 00:13:08,923 ----------------------------------------------------------------------------------------------------
2025-06-27 00:13:08,923 Plugins:
2025-06-27 00:13:08,923  - AnnealOnPlateau | patience: '3', anneal_factor: '0.5', min_learning_rate: '0.0001'
2025-06-27 00:13:08,923 ----------------------------------------------------------------------------------------------------
2025-06-27 00:13:08,923 Final evaluation on model from best epoch (best-model.pt)
2025-06-27 00:13:08,923  - metric: "('micro avg', 'f1-score')"
2025-06-27 00:13:08,923 ----------------------------------------------------------------------------------------------------
2025-06-27 00:13:08,923 Computation:
2025-06-27 00:13:08,923  - compute on device: cuda:0
2025-06-27 00:13:08,923  - embedding storage: cpu
2025-06-27 00:13:08,923 ----------------------------------------------------------------------------------------------------
2025-06-27 00:13:08,923 Model training base path: "resources/taggers/ner-bertimbau"
2025-06-27 00:13:08,923 ----------------------------------------------------------------------------------------------------
2025-06-27 00:13:08,923 ----------------------------------------------------------------------------------------------------
model.safetensors: 100% 438M/438M [00:01<00:00, 357MB/s]
2025-06-27 00:15:41,667 epoch 1 - iter 1400/14007 - loss 3.06478238 - time (sec): 152.74 - samples/sec: 1215.09 - lr: 0.000200 - momentum: 0.000000
2025-06-27 00:18:20,932 epoch 1 - iter 2800/14007 - loss 2.76504276 - time (sec): 312.01 - samples/sec: 1198.26 - lr: 0.000200 - momentum: 0.000000
2025-06-27 00:20:55,858 epoch 1 - iter 4200/14007 - loss 2.57746831 - time (sec): 466.93 - samples/sec: 1201.46 - lr: 0.000200 - momentum: 0.000000
2025-06-27 00:23:31,861 epoch 1 - iter 5600/14007 - loss 2.44704030 - time (sec): 622.94 - samples/sec: 1200.68 - lr: 0.000200 - momentum: 0.000000
2025-06-27 00:26:07,289 epoch 1 - iter 7000/14007 - loss 2.34757380 - time (sec): 778.36 - samples/sec: 1200.59 - lr: 0.000200 - momentum: 0.000000
2025-06-27 00:28:42,114 epoch 1 - iter 8400/14007 - loss 2.26385339 - time (sec): 933.19 - samples/sec: 1201.46 - lr: 0.000200 - momentum: 0.000000
2025-06-27 00:31:19,969 epoch 1 - iter 9800/14007 - loss 2.19187111 - time (sec): 1091.04 - samples/sec: 1199.20 - lr: 0.000200 - momentum: 0.000000
2025-06-27 00:33:56,181 epoch 1 - iter 11200/14007 - loss 2.12548202 - time (sec): 1247.26 - samples/sec: 1199.22 - lr: 0.000200 - momentum: 0.000000
2025-06-27 00:36:32,514 epoch 1 - iter 12600/14007 - loss 2.06537266 - time (sec): 1403.59 - samples/sec: 1198.06 - lr: 0.000200 - momentum: 0.000000
2025-06-27 00:39:08,478 epoch 1 - iter 14000/14007 - loss 2.00840135 - time (sec): 1559.55 - samples/sec: 1199.12 - lr: 0.000200 - momentum: 0.000000
2025-06-27 00:39:09,257 ----------------------------------------------------------------------------------------------------
2025-06-27 00:39:09,257 EPOCH 1 done: loss 2.0081 - lr: 0.000200
100% 438/438 [01:49<00:00,  3.99it/s]
2025-06-27 00:40:59,702 DEV : loss 1.2904926538467407 - f1-score (micro avg)  0.3198
2025-06-27 00:41:00,040  - 0 epochs without improvement
2025-06-27 00:41:00,040 saving best model
2025-06-27 00:41:00,614 ----------------------------------------------------------------------------------------------------
2025-06-27 00:43:36,464 epoch 2 - iter 1400/14007 - loss 1.42188469 - time (sec): 155.85 - samples/sec: 1204.84 - lr: 0.000200 - momentum: 0.000000
2025-06-27 00:46:17,362 epoch 2 - iter 2800/14007 - loss 1.39736893 - time (sec): 316.75 - samples/sec: 1184.02 - lr: 0.000200 - momentum: 0.000000
2025-06-27 00:48:57,680 epoch 2 - iter 4200/14007 - loss 1.36602578 - time (sec): 477.06 - samples/sec: 1178.72 - lr: 0.000200 - momentum: 0.000000
2025-06-27 00:51:37,245 epoch 2 - iter 5600/14007 - loss 1.33517505 - time (sec): 636.63 - samples/sec: 1176.92 - lr: 0.000200 - momentum: 0.000000
2025-06-27 00:54:15,139 epoch 2 - iter 7000/14007 - loss 1.30587100 - time (sec): 794.52 - samples/sec: 1178.32 - lr: 0.000200 - momentum: 0.000000
2025-06-27 00:56:54,490 epoch 2 - iter 8400/14007 - loss 1.27728485 - time (sec): 953.88 - samples/sec: 1177.07 - lr: 0.000200 - momentum: 0.000000
2025-06-27 00:59:34,424 epoch 2 - iter 9800/14007 - loss 1.24804019 - time (sec): 1113.81 - samples/sec: 1176.11 - lr: 0.000200 - momentum: 0.000000
2025-06-27 01:02:15,219 epoch 2 - iter 11200/14007 - loss 1.22341434 - time (sec): 1274.60 - samples/sec: 1174.38 - lr: 0.000200 - momentum: 0.000000
2025-06-27 01:04:54,741 epoch 2 - iter 12600/14007 - loss 1.19794726 - time (sec): 1434.13 - samples/sec: 1173.99 - lr: 0.000200 - momentum: 0.000000
2025-06-27 01:07:34,133 epoch 2 - iter 14000/14007 - loss 1.17408773 - time (sec): 1593.52 - samples/sec: 1173.61 - lr: 0.000200 - momentum: 0.000000
2025-06-27 01:07:34,859 ----------------------------------------------------------------------------------------------------
2025-06-27 01:07:34,859 EPOCH 2 done: loss 1.1740 - lr: 0.000200
100% 438/438 [00:52<00:00,  8.36it/s]
2025-06-27 01:08:27,875 DEV : loss 0.9252111315727234 - f1-score (micro avg)  0.5887
2025-06-27 01:08:28,232  - 0 epochs without improvement
2025-06-27 01:08:28,232 saving best model
2025-06-27 01:08:29,100 ----------------------------------------------------------------------------------------------------
2025-06-27 01:11:10,634 epoch 3 - iter 1400/14007 - loss 0.94223489 - time (sec): 161.53 - samples/sec: 1156.30 - lr: 0.000200 - momentum: 0.000000
2025-06-27 01:13:53,821 epoch 3 - iter 2800/14007 - loss 0.92145402 - time (sec): 324.72 - samples/sec: 1152.31 - lr: 0.000200 - momentum: 0.000000
2025-06-27 01:16:34,833 epoch 3 - iter 4200/14007 - loss 0.90413855 - time (sec): 485.73 - samples/sec: 1153.27 - lr: 0.000200 - momentum: 0.000000
2025-06-27 01:19:15,349 epoch 3 - iter 5600/14007 - loss 0.88762133 - time (sec): 646.25 - samples/sec: 1156.38 - lr: 0.000200 - momentum: 0.000000
2025-06-27 01:21:57,583 epoch 3 - iter 7000/14007 - loss 0.87401487 - time (sec): 808.48 - samples/sec: 1155.13 - lr: 0.000200 - momentum: 0.000000
2025-06-27 01:24:37,440 epoch 3 - iter 8400/14007 - loss 0.85982111 - time (sec): 968.34 - samples/sec: 1157.53 - lr: 0.000200 - momentum: 0.000000
2025-06-27 01:27:19,250 epoch 3 - iter 9800/14007 - loss 0.84596086 - time (sec): 1130.15 - samples/sec: 1157.39 - lr: 0.000200 - momentum: 0.000000
2025-06-27 01:30:01,109 epoch 3 - iter 11200/14007 - loss 0.83346140 - time (sec): 1292.01 - samples/sec: 1157.13 - lr: 0.000200 - momentum: 0.000000
2025-06-27 01:32:40,898 epoch 3 - iter 12600/14007 - loss 0.82198304 - time (sec): 1451.80 - samples/sec: 1157.97 - lr: 0.000200 - momentum: 0.000000
2025-06-27 01:35:29,140 epoch 3 - iter 14000/14007 - loss 0.81106675 - time (sec): 1620.04 - samples/sec: 1154.39 - lr: 0.000200 - momentum: 0.000000
2025-06-27 01:35:29,843 ----------------------------------------------------------------------------------------------------
2025-06-27 01:35:29,843 EPOCH 3 done: loss 0.8110 - lr: 0.000200
100% 438/438 [00:42<00:00, 10.41it/s]
2025-06-27 01:36:12,490 DEV : loss 0.7913607954978943 - f1-score (micro avg)  0.6919
2025-06-27 01:36:12,837  - 0 epochs without improvement
2025-06-27 01:36:12,837 saving best model
2025-06-27 01:36:13,778 ----------------------------------------------------------------------------------------------------
2025-06-27 01:38:53,539 epoch 4 - iter 1400/14007 - loss 0.70394951 - time (sec): 159.76 - samples/sec: 1172.03 - lr: 0.000200 - momentum: 0.000000
2025-06-27 01:41:35,403 epoch 4 - iter 2800/14007 - loss 0.68619685 - time (sec): 321.62 - samples/sec: 1166.78 - lr: 0.000200 - momentum: 0.000000
2025-06-27 01:44:17,568 epoch 4 - iter 4200/14007 - loss 0.67798854 - time (sec): 483.79 - samples/sec: 1160.95 - lr: 0.000200 - momentum: 0.000000
2025-06-27 01:46:56,263 epoch 4 - iter 5600/14007 - loss 0.67057505 - time (sec): 642.48 - samples/sec: 1164.70 - lr: 0.000200 - momentum: 0.000000
2025-06-27 01:49:38,716 epoch 4 - iter 7000/14007 - loss 0.66140954 - time (sec): 804.94 - samples/sec: 1163.24 - lr: 0.000200 - momentum: 0.000000
2025-06-27 01:52:22,613 epoch 4 - iter 8400/14007 - loss 0.65436540 - time (sec): 968.83 - samples/sec: 1159.27 - lr: 0.000200 - momentum: 0.000000
2025-06-27 01:55:03,748 epoch 4 - iter 9800/14007 - loss 0.64603148 - time (sec): 1129.97 - samples/sec: 1159.79 - lr: 0.000200 - momentum: 0.000000
2025-06-27 01:57:44,732 epoch 4 - iter 11200/14007 - loss 0.64093941 - time (sec): 1290.95 - samples/sec: 1159.33 - lr: 0.000200 - momentum: 0.000000
2025-06-27 02:00:31,187 epoch 4 - iter 12600/14007 - loss 0.63432262 - time (sec): 1457.41 - samples/sec: 1154.46 - lr: 0.000200 - momentum: 0.000000
2025-06-27 02:03:13,203 epoch 4 - iter 14000/14007 - loss 0.62890196 - time (sec): 1619.42 - samples/sec: 1154.82 - lr: 0.000200 - momentum: 0.000000
2025-06-27 02:03:14,008 ----------------------------------------------------------------------------------------------------
2025-06-27 02:03:14,008 EPOCH 4 done: loss 0.6289 - lr: 0.000200
100% 438/438 [00:45<00:00,  9.61it/s]
2025-06-27 02:04:00,172 DEV : loss 0.7236550450325012 - f1-score (micro avg)  0.7322
2025-06-27 02:04:00,535  - 0 epochs without improvement
2025-06-27 02:04:00,535 saving best model
2025-06-27 02:04:01,510 ----------------------------------------------------------------------------------------------------
2025-06-27 02:06:47,331 epoch 5 - iter 1400/14007 - loss 0.56911206 - time (sec): 165.82 - samples/sec: 1135.67 - lr: 0.000200 - momentum: 0.000000
2025-06-27 02:09:29,565 epoch 5 - iter 2800/14007 - loss 0.55963918 - time (sec): 328.05 - samples/sec: 1142.76 - lr: 0.000200 - momentum: 0.000000
2025-06-27 02:12:11,328 epoch 5 - iter 4200/14007 - loss 0.55454435 - time (sec): 489.82 - samples/sec: 1148.46 - lr: 0.000200 - momentum: 0.000000
2025-06-27 02:14:54,646 epoch 5 - iter 5600/14007 - loss 0.54810696 - time (sec): 653.13 - samples/sec: 1147.53 - lr: 0.000200 - momentum: 0.000000
2025-06-27 02:17:37,399 epoch 5 - iter 7000/14007 - loss 0.54183125 - time (sec): 815.89 - samples/sec: 1148.92 - lr: 0.000200 - momentum: 0.000000
2025-06-27 02:20:22,503 epoch 5 - iter 8400/14007 - loss 0.53849657 - time (sec): 980.99 - samples/sec: 1145.62 - lr: 0.000200 - momentum: 0.000000
2025-06-27 02:22:59,709 epoch 5 - iter 9800/14007 - loss 0.53420978 - time (sec): 1138.20 - samples/sec: 1150.94 - lr: 0.000200 - momentum: 0.000000
2025-06-27 02:25:38,950 epoch 5 - iter 11200/14007 - loss 0.52963751 - time (sec): 1297.44 - samples/sec: 1153.27 - lr: 0.000200 - momentum: 0.000000
2025-06-27 02:28:23,255 epoch 5 - iter 12600/14007 - loss 0.52682475 - time (sec): 1461.74 - samples/sec: 1151.36 - lr: 0.000200 - momentum: 0.000000
2025-06-27 02:31:04,589 epoch 5 - iter 14000/14007 - loss 0.52210996 - time (sec): 1623.08 - samples/sec: 1152.19 - lr: 0.000200 - momentum: 0.000000
2025-06-27 02:31:05,372 ----------------------------------------------------------------------------------------------------
2025-06-27 02:31:05,372 EPOCH 5 done: loss 0.5221 - lr: 0.000200
100% 438/438 [00:45<00:00,  9.56it/s]
2025-06-27 02:31:58,853 DEV : loss 0.6808485984802246 - f1-score (micro avg)  0.7426
2025-06-27 02:31:59,247  - 0 epochs without improvement
2025-06-27 02:31:59,247 saving best model
2025-06-27 02:32:00,131 ----------------------------------------------------------------------------------------------------
2025-06-27 02:34:42,400 epoch 6 - iter 1400/14007 - loss 0.47603135 - time (sec): 162.27 - samples/sec: 1150.65 - lr: 0.000200 - momentum: 0.000000
2025-06-27 02:37:24,899 epoch 6 - iter 2800/14007 - loss 0.47854668 - time (sec): 324.77 - samples/sec: 1152.63 - lr: 0.000200 - momentum: 0.000000
2025-06-27 02:40:07,450 epoch 6 - iter 4200/14007 - loss 0.46993878 - time (sec): 487.32 - samples/sec: 1151.32 - lr: 0.000200 - momentum: 0.000000
2025-06-27 02:42:48,250 epoch 6 - iter 5600/14007 - loss 0.46709702 - time (sec): 648.12 - samples/sec: 1154.46 - lr: 0.000200 - momentum: 0.000000
2025-06-27 02:45:30,587 epoch 6 - iter 7000/14007 - loss 0.46334752 - time (sec): 810.46 - samples/sec: 1153.31 - lr: 0.000200 - momentum: 0.000000
2025-06-27 02:48:12,539 epoch 6 - iter 8400/14007 - loss 0.46034075 - time (sec): 972.41 - samples/sec: 1153.14 - lr: 0.000200 - momentum: 0.000000
2025-06-27 02:50:56,217 epoch 6 - iter 9800/14007 - loss 0.45919342 - time (sec): 1136.08 - samples/sec: 1151.98 - lr: 0.000200 - momentum: 0.000000
2025-06-27 02:53:41,808 epoch 6 - iter 11200/14007 - loss 0.45749574 - time (sec): 1301.68 - samples/sec: 1149.35 - lr: 0.000200 - momentum: 0.000000
2025-06-27 02:56:22,344 epoch 6 - iter 12600/14007 - loss 0.45465355 - time (sec): 1462.21 - samples/sec: 1151.26 - lr: 0.000200 - momentum: 0.000000
2025-06-27 02:59:04,226 epoch 6 - iter 14000/14007 - loss 0.45132602 - time (sec): 1624.09 - samples/sec: 1151.49 - lr: 0.000200 - momentum: 0.000000
2025-06-27 02:59:05,003 ----------------------------------------------------------------------------------------------------
2025-06-27 02:59:05,004 EPOCH 6 done: loss 0.4513 - lr: 0.000200
100% 438/438 [00:52<00:00,  8.30it/s]
2025-06-27 02:59:58,410 DEV : loss 0.6541404724121094 - f1-score (micro avg)  0.7391
2025-06-27 02:59:58,773  - 1 epochs without improvement
2025-06-27 02:59:58,773 ----------------------------------------------------------------------------------------------------
2025-06-27 03:02:40,102 epoch 7 - iter 1400/14007 - loss 0.41915772 - time (sec): 161.33 - samples/sec: 1160.29 - lr: 0.000200 - momentum: 0.000000
2025-06-27 03:05:26,367 epoch 7 - iter 2800/14007 - loss 0.42050990 - time (sec): 327.59 - samples/sec: 1143.27 - lr: 0.000200 - momentum: 0.000000
2025-06-27 03:08:08,693 epoch 7 - iter 4200/14007 - loss 0.41472582 - time (sec): 489.92 - samples/sec: 1144.54 - lr: 0.000200 - momentum: 0.000000
2025-06-27 03:10:51,007 epoch 7 - iter 5600/14007 - loss 0.41404891 - time (sec): 652.23 - samples/sec: 1147.95 - lr: 0.000200 - momentum: 0.000000
2025-06-27 03:13:32,746 epoch 7 - iter 7000/14007 - loss 0.40918977 - time (sec): 813.97 - samples/sec: 1149.34 - lr: 0.000200 - momentum: 0.000000
2025-06-27 03:16:15,167 epoch 7 - iter 8400/14007 - loss 0.40583145 - time (sec): 976.39 - samples/sec: 1150.28 - lr: 0.000200 - momentum: 0.000000
2025-06-27 03:18:55,961 epoch 7 - iter 9800/14007 - loss 0.40502011 - time (sec): 1137.19 - samples/sec: 1151.21 - lr: 0.000200 - momentum: 0.000000
2025-06-27 03:21:36,308 epoch 7 - iter 11200/14007 - loss 0.40246238 - time (sec): 1297.53 - samples/sec: 1151.82 - lr: 0.000200 - momentum: 0.000000
2025-06-27 03:24:18,488 epoch 7 - iter 12600/14007 - loss 0.40195364 - time (sec): 1459.71 - samples/sec: 1152.26 - lr: 0.000200 - momentum: 0.000000
2025-06-27 03:26:57,927 epoch 7 - iter 14000/14007 - loss 0.40037306 - time (sec): 1619.15 - samples/sec: 1154.94 - lr: 0.000200 - momentum: 0.000000
2025-06-27 03:26:58,757 ----------------------------------------------------------------------------------------------------
2025-06-27 03:26:58,757 EPOCH 7 done: loss 0.4004 - lr: 0.000200
100% 438/438 [00:50<00:00,  8.64it/s]
2025-06-27 03:27:50,026 DEV : loss 0.6374567747116089 - f1-score (micro avg)  0.751
2025-06-27 03:27:50,364  - 0 epochs without improvement
2025-06-27 03:27:50,364 saving best model
2025-06-27 03:27:51,193 ----------------------------------------------------------------------------------------------------
2025-06-27 03:30:31,713 epoch 8 - iter 1400/14007 - loss 0.37794191 - time (sec): 160.52 - samples/sec: 1176.51 - lr: 0.000200 - momentum: 0.000000
2025-06-27 03:33:11,338 epoch 8 - iter 2800/14007 - loss 0.37559030 - time (sec): 320.14 - samples/sec: 1172.96 - lr: 0.000200 - momentum: 0.000000
2025-06-27 03:35:54,189 epoch 8 - iter 4200/14007 - loss 0.38040309 - time (sec): 483.00 - samples/sec: 1163.82 - lr: 0.000200 - momentum: 0.000000
2025-06-27 03:38:35,690 epoch 8 - iter 5600/14007 - loss 0.37656978 - time (sec): 644.50 - samples/sec: 1161.09 - lr: 0.000200 - momentum: 0.000000
2025-06-27 03:41:15,418 epoch 8 - iter 7000/14007 - loss 0.37434086 - time (sec): 804.22 - samples/sec: 1161.53 - lr: 0.000200 - momentum: 0.000000
2025-06-27 03:43:53,716 epoch 8 - iter 8400/14007 - loss 0.37184711 - time (sec): 962.52 - samples/sec: 1164.09 - lr: 0.000200 - momentum: 0.000000
2025-06-27 03:46:36,686 epoch 8 - iter 9800/14007 - loss 0.36882296 - time (sec): 1125.49 - samples/sec: 1162.87 - lr: 0.000200 - momentum: 0.000000
2025-06-27 03:49:17,775 epoch 8 - iter 11200/14007 - loss 0.36640248 - time (sec): 1286.58 - samples/sec: 1162.78 - lr: 0.000200 - momentum: 0.000000
2025-06-27 03:51:58,654 epoch 8 - iter 12600/14007 - loss 0.36431310 - time (sec): 1447.46 - samples/sec: 1162.66 - lr: 0.000200 - momentum: 0.000000
2025-06-27 03:54:37,034 epoch 8 - iter 14000/14007 - loss 0.36329935 - time (sec): 1605.84 - samples/sec: 1164.52 - lr: 0.000200 - momentum: 0.000000
2025-06-27 03:54:37,799 ----------------------------------------------------------------------------------------------------
2025-06-27 03:54:37,799 EPOCH 8 done: loss 0.3633 - lr: 0.000200
100% 438/438 [00:50<00:00,  8.64it/s]
2025-06-27 03:55:29,033 DEV : loss 0.6278331279754639 - f1-score (micro avg)  0.7683
2025-06-27 03:55:29,385  - 0 epochs without improvement
2025-06-27 03:55:29,385 saving best model
2025-06-27 03:55:30,239 ----------------------------------------------------------------------------------------------------
2025-06-27 03:58:11,424 epoch 9 - iter 1400/14007 - loss 0.33828079 - time (sec): 161.18 - samples/sec: 1159.24 - lr: 0.000200 - momentum: 0.000000
2025-06-27 04:00:52,904 epoch 9 - iter 2800/14007 - loss 0.33972098 - time (sec): 322.66 - samples/sec: 1160.61 - lr: 0.000200 - momentum: 0.000000
2025-06-27 04:03:33,067 epoch 9 - iter 4200/14007 - loss 0.33825093 - time (sec): 482.83 - samples/sec: 1163.91 - lr: 0.000200 - momentum: 0.000000
2025-06-27 04:06:14,404 epoch 9 - iter 5600/14007 - loss 0.33823772 - time (sec): 644.16 - samples/sec: 1160.78 - lr: 0.000200 - momentum: 0.000000
2025-06-27 04:08:55,693 epoch 9 - iter 7000/14007 - loss 0.33696090 - time (sec): 805.45 - samples/sec: 1159.93 - lr: 0.000200 - momentum: 0.000000
2025-06-27 04:11:36,301 epoch 9 - iter 8400/14007 - loss 0.33587798 - time (sec): 966.06 - samples/sec: 1161.26 - lr: 0.000200 - momentum: 0.000000
2025-06-27 04:14:15,888 epoch 9 - iter 9800/14007 - loss 0.33485940 - time (sec): 1125.65 - samples/sec: 1162.40 - lr: 0.000200 - momentum: 0.000000
2025-06-27 04:17:02,962 epoch 9 - iter 11200/14007 - loss 0.33653454 - time (sec): 1292.72 - samples/sec: 1156.85 - lr: 0.000200 - momentum: 0.000000
2025-06-27 04:19:44,626 epoch 9 - iter 12600/14007 - loss 0.33613782 - time (sec): 1454.39 - samples/sec: 1157.43 - lr: 0.000200 - momentum: 0.000000
2025-06-27 04:22:23,056 epoch 9 - iter 14000/14007 - loss 0.33459644 - time (sec): 1612.82 - samples/sec: 1159.59 - lr: 0.000200 - momentum: 0.000000
2025-06-27 04:22:23,770 ----------------------------------------------------------------------------------------------------
2025-06-27 04:22:23,771 EPOCH 9 done: loss 0.3346 - lr: 0.000200
100% 438/438 [00:41<00:00, 10.53it/s]
2025-06-27 04:23:05,895 DEV : loss 0.6212080717086792 - f1-score (micro avg)  0.7786
2025-06-27 04:23:06,235  - 0 epochs without improvement
2025-06-27 04:23:06,235 saving best model
2025-06-27 04:23:07,062 ----------------------------------------------------------------------------------------------------
2025-06-27 04:25:45,696 epoch 10 - iter 1400/14007 - loss 0.31513031 - time (sec): 158.63 - samples/sec: 1182.83 - lr: 0.000200 - momentum: 0.000000
2025-06-27 04:28:23,500 epoch 10 - iter 2800/14007 - loss 0.32211233 - time (sec): 316.44 - samples/sec: 1185.85 - lr: 0.000200 - momentum: 0.000000
2025-06-27 04:31:03,479 epoch 10 - iter 4200/14007 - loss 0.31669534 - time (sec): 476.42 - samples/sec: 1180.85 - lr: 0.000200 - momentum: 0.000000
2025-06-27 04:33:47,910 epoch 10 - iter 5600/14007 - loss 0.31616989 - time (sec): 640.85 - samples/sec: 1167.50 - lr: 0.000200 - momentum: 0.000000
2025-06-27 04:36:28,080 epoch 10 - iter 7000/14007 - loss 0.31706240 - time (sec): 801.02 - samples/sec: 1167.85 - lr: 0.000200 - momentum: 0.000000
2025-06-27 04:39:05,455 epoch 10 - iter 8400/14007 - loss 0.31649791 - time (sec): 958.39 - samples/sec: 1170.32 - lr: 0.000200 - momentum: 0.000000
2025-06-27 04:41:44,316 epoch 10 - iter 9800/14007 - loss 0.31464984 - time (sec): 1117.25 - samples/sec: 1171.48 - lr: 0.000200 - momentum: 0.000000
2025-06-27 04:44:21,882 epoch 10 - iter 11200/14007 - loss 0.31347566 - time (sec): 1274.82 - samples/sec: 1172.63 - lr: 0.000200 - momentum: 0.000000
2025-06-27 04:47:01,770 epoch 10 - iter 12600/14007 - loss 0.31353836 - time (sec): 1434.71 - samples/sec: 1172.59 - lr: 0.000200 - momentum: 0.000000
2025-06-27 04:49:39,070 epoch 10 - iter 14000/14007 - loss 0.31167372 - time (sec): 1592.01 - samples/sec: 1174.66 - lr: 0.000200 - momentum: 0.000000
2025-06-27 04:49:39,865 ----------------------------------------------------------------------------------------------------
2025-06-27 04:49:39,866 EPOCH 10 done: loss 0.3117 - lr: 0.000200
100% 438/438 [00:49<00:00,  8.84it/s]
2025-06-27 04:50:29,979 DEV : loss 0.6171913146972656 - f1-score (micro avg)  0.7839
2025-06-27 04:50:30,328  - 0 epochs without improvement
2025-06-27 04:50:30,328 saving best model
2025-06-27 04:50:31,912 ----------------------------------------------------------------------------------------------------
2025-06-27 04:50:31,914 Loading model from best epoch ...
2025-06-27 04:50:34,218 SequenceTagger predicts: Dictionary with 27 tags: O, S-LOCALIZACAO, B-LOCALIZACAO, E-LOCALIZACAO, I-LOCALIZACAO, S-SETOR, B-SETOR, E-SETOR, I-SETOR, S-PORTE, B-PORTE, E-PORTE, I-PORTE, S-NOME_EMPRESA, B-NOME_EMPRESA, E-NOME_EMPRESA, I-NOME_EMPRESA, S-QTD_FUNCIONARIOS, B-QTD_FUNCIONARIOS, E-QTD_FUNCIONARIOS, I-QTD_FUNCIONARIOS, S-FATURAMENTO, B-FATURAMENTO, E-FATURAMENTO, I-FATURAMENTO, <START>, <STOP>
100% 438/438 [01:24<00:00,  5.21it/s]
2025-06-27 04:51:58,916 
Results:
- F-score (micro) 0.9324
- F-score (macro) 0.9446
- Accuracy 0.8775

By class:
                  precision    recall  f1-score   support

     LOCALIZACAO     0.9694    0.9704    0.9699     16405
           SETOR     0.9485    0.9254    0.9369     15478
           PORTE     0.9964    0.9882    0.9923     13874
    NOME_EMPRESA     0.7583    0.7983    0.7778     13173
QTD_FUNCIONARIOS     0.9898    0.9992    0.9945      5323
     FATURAMENTO     0.9943    0.9990    0.9966      3836

       micro avg     0.9304    0.9344    0.9324     68089
       macro avg     0.9428    0.9468    0.9446     68089
    weighted avg     0.9323    0.9344    0.9332     68089

2025-06-27 04:51:58,916 ----------------------------------------------------------------------------------------------------

