Already up to date.
2025-06-22 06:29:51,218 Reading data from datasets/flair
2025-06-22 06:29:51,218 Train: datasets/flair/train.txt
2025-06-22 06:29:51,218 Dev: datasets/flair/dev.txt
2025-06-22 06:29:51,218 Test: datasets/flair/test.txt
tokenizer_config.json: 100% 43.0/43.0 [00:00<00:00, 335kB/s]
config.json: 100% 647/647 [00:00<00:00, 5.03MB/s]
vocab.txt: 100% 210k/210k [00:00<00:00, 956kB/s]
added_tokens.json: 100% 2.00/2.00 [00:00<00:00, 16.5kB/s]
special_tokens_map.json: 100% 112/112 [00:00<00:00, 991kB/s]
2025-06-22 06:30:33,215 Computing label dictionary. Progress:
0it [00:00, ?it/s]
168253it [00:03, 44533.59it/s]
2025-06-22 06:30:36,995 Dictionary created for label 'ner' with 4 values: LOCALIZACAO (seen 109701 times), NOME_EMPRESA (seen 106779 times), SETOR (seen 88492 times), PORTE (seen 81971 times)
pytorch_model.bin: 100% 438M/438M [00:01<00:00, 312MB/s]
2025-06-22 06:30:41,984 SequenceTagger predicts: Dictionary with 17 tags: O, S-LOCALIZACAO, B-LOCALIZACAO, E-LOCALIZACAO, I-LOCALIZACAO, S-NOME_EMPRESA, B-NOME_EMPRESA, E-NOME_EMPRESA, I-NOME_EMPRESA, S-SETOR, B-SETOR, E-SETOR, I-SETOR, S-PORTE, B-PORTE, E-PORTE, I-PORTE
/root/.cache/pypoetry/virtualenvs/b2ner-kQ3VDw2i-py3.11/lib/python3.11/site-packages/flair/trainers/trainer.py:545: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=use_amp and flair.device.type != "cpu")
2025-06-22 06:30:42,009 ----------------------------------------------------------------------------------------------------
2025-06-22 06:30:42,010 Model: "SequenceTagger(
  (embeddings): TransformerWordEmbeddings(
    (model): BertModel(
      (embeddings): BertEmbeddings(
        (word_embeddings): Embedding(29795, 768, padding_idx=0)
        (position_embeddings): Embedding(512, 768)
        (token_type_embeddings): Embedding(2, 768)
        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (encoder): BertEncoder(
        (layer): ModuleList(
          (0-11): 12 x BertLayer(
            (attention): BertAttention(
              (self): BertSdpaSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
        )
      )
      (pooler): BertPooler(
        (dense): Linear(in_features=768, out_features=768, bias=True)
        (activation): Tanh()
      )
    )
  )
  (word_dropout): WordDropout(p=0.05)
  (locked_dropout): LockedDropout(p=0.5)
  (embedding2nn): Linear(in_features=768, out_features=768, bias=True)
  (rnn): LSTM(768, 128, batch_first=True, bidirectional=True)
  (linear): Linear(in_features=256, out_features=19, bias=True)
  (loss_function): ViterbiLoss()
  (crf): CRF()
)"
2025-06-22 06:30:42,010 ----------------------------------------------------------------------------------------------------
2025-06-22 06:30:42,010 Corpus: 168253 train + 21032 dev + 21029 test sentences
2025-06-22 06:30:42,010 ----------------------------------------------------------------------------------------------------
2025-06-22 06:30:42,010 Train:  168253 sentences
2025-06-22 06:30:42,010         (train_with_dev=False, train_with_test=False)
2025-06-22 06:30:42,010 ----------------------------------------------------------------------------------------------------
2025-06-22 06:30:42,010 Training Params:
2025-06-22 06:30:42,010  - learning_rate: "0.0002" 
2025-06-22 06:30:42,010  - mini_batch_size: "8"
2025-06-22 06:30:42,010  - max_epochs: "10"
2025-06-22 06:30:42,010  - shuffle: "True"
2025-06-22 06:30:42,010 ----------------------------------------------------------------------------------------------------
2025-06-22 06:30:42,010 Plugins:
2025-06-22 06:30:42,010  - AnnealOnPlateau | patience: '3', anneal_factor: '0.5', min_learning_rate: '0.0001'
2025-06-22 06:30:42,010 ----------------------------------------------------------------------------------------------------
2025-06-22 06:30:42,010 Final evaluation on model from best epoch (best-model.pt)
2025-06-22 06:30:42,010  - metric: "('micro avg', 'f1-score')"
2025-06-22 06:30:42,010 ----------------------------------------------------------------------------------------------------
2025-06-22 06:30:42,010 Computation:
2025-06-22 06:30:42,010  - compute on device: cuda:0
2025-06-22 06:30:42,010  - embedding storage: cpu
2025-06-22 06:30:42,010 ----------------------------------------------------------------------------------------------------
2025-06-22 06:30:42,011 Model training base path: "resources/taggers/ner-bertimbau"
2025-06-22 06:30:42,011 ----------------------------------------------------------------------------------------------------
2025-06-22 06:30:42,011 ----------------------------------------------------------------------------------------------------
model.safetensors: 100% 438M/438M [00:01<00:00, 355MB/s]
2025-06-22 06:32:58,294 epoch 1 - iter 2103/21032 - loss 2.89041295 - time (sec): 136.28 - samples/sec: 878.35 - lr: 0.000200 - momentum: 0.000000
2025-06-22 06:35:14,252 epoch 1 - iter 4206/21032 - loss 2.70202527 - time (sec): 272.24 - samples/sec: 876.93 - lr: 0.000200 - momentum: 0.000000
2025-06-22 06:37:31,087 epoch 1 - iter 6309/21032 - loss 2.55357084 - time (sec): 409.08 - samples/sec: 875.46 - lr: 0.000200 - momentum: 0.000000
2025-06-22 06:39:49,611 epoch 1 - iter 8412/21032 - loss 2.42314032 - time (sec): 547.60 - samples/sec: 871.59 - lr: 0.000200 - momentum: 0.000000
2025-06-22 06:42:07,586 epoch 1 - iter 10515/21032 - loss 2.30910007 - time (sec): 685.57 - samples/sec: 869.50 - lr: 0.000200 - momentum: 0.000000
2025-06-22 06:44:25,177 epoch 1 - iter 12618/21032 - loss 2.20893673 - time (sec): 823.17 - samples/sec: 868.58 - lr: 0.000200 - momentum: 0.000000
2025-06-22 06:46:41,783 epoch 1 - iter 14721/21032 - loss 2.12106416 - time (sec): 959.77 - samples/sec: 868.85 - lr: 0.000200 - momentum: 0.000000
2025-06-22 06:49:00,096 epoch 1 - iter 16824/21032 - loss 2.04289019 - time (sec): 1098.08 - samples/sec: 867.75 - lr: 0.000200 - momentum: 0.000000
2025-06-22 06:51:17,021 epoch 1 - iter 18927/21032 - loss 1.97381535 - time (sec): 1235.01 - samples/sec: 866.83 - lr: 0.000200 - momentum: 0.000000
2025-06-22 06:53:33,455 epoch 1 - iter 21030/21032 - loss 1.91131893 - time (sec): 1371.44 - samples/sec: 867.32 - lr: 0.000200 - momentum: 0.000000
2025-06-22 06:53:33,582 ----------------------------------------------------------------------------------------------------
2025-06-22 06:53:33,582 EPOCH 1 done: loss 1.9113 - lr: 0.000200
100% 329/329 [01:17<00:00,  4.25it/s]
2025-06-22 06:54:51,311 DEV : loss 1.1281368732452393 - f1-score (micro avg)  0.4941
2025-06-22 06:54:51,535  - 0 epochs without improvement
2025-06-22 06:54:51,535 saving best model
2025-06-22 06:54:52,102 ----------------------------------------------------------------------------------------------------
2025-06-22 06:57:10,282 epoch 2 - iter 2103/21032 - loss 1.27126781 - time (sec): 138.18 - samples/sec: 864.07 - lr: 0.000200 - momentum: 0.000000
2025-06-22 06:59:27,918 epoch 2 - iter 4206/21032 - loss 1.24003919 - time (sec): 275.81 - samples/sec: 863.92 - lr: 0.000200 - momentum: 0.000000
2025-06-22 07:01:46,102 epoch 2 - iter 6309/21032 - loss 1.21321130 - time (sec): 414.00 - samples/sec: 862.81 - lr: 0.000200 - momentum: 0.000000
2025-06-22 07:04:04,409 epoch 2 - iter 8412/21032 - loss 1.18674810 - time (sec): 552.31 - samples/sec: 861.73 - lr: 0.000200 - momentum: 0.000000
2025-06-22 07:06:22,643 epoch 2 - iter 10515/21032 - loss 1.15960512 - time (sec): 690.54 - samples/sec: 861.32 - lr: 0.000200 - momentum: 0.000000
2025-06-22 07:08:42,737 epoch 2 - iter 12618/21032 - loss 1.13382458 - time (sec): 830.63 - samples/sec: 859.22 - lr: 0.000200 - momentum: 0.000000
2025-06-22 07:11:01,500 epoch 2 - iter 14721/21032 - loss 1.10855211 - time (sec): 969.40 - samples/sec: 859.57 - lr: 0.000200 - momentum: 0.000000
2025-06-22 07:13:19,636 epoch 2 - iter 16824/21032 - loss 1.08549617 - time (sec): 1107.53 - samples/sec: 859.77 - lr: 0.000200 - momentum: 0.000000
2025-06-22 07:15:40,674 epoch 2 - iter 18927/21032 - loss 1.06453612 - time (sec): 1248.57 - samples/sec: 858.04 - lr: 0.000200 - momentum: 0.000000
2025-06-22 07:17:59,541 epoch 2 - iter 21030/21032 - loss 1.04535212 - time (sec): 1387.44 - samples/sec: 857.34 - lr: 0.000200 - momentum: 0.000000
2025-06-22 07:17:59,657 ----------------------------------------------------------------------------------------------------
2025-06-22 07:17:59,657 EPOCH 2 done: loss 1.0454 - lr: 0.000200
100% 329/329 [00:31<00:00, 10.40it/s]
2025-06-22 07:18:31,618 DEV : loss 0.8664954304695129 - f1-score (micro avg)  0.7568
2025-06-22 07:18:31,839  - 0 epochs without improvement
2025-06-22 07:18:31,839 saving best model
2025-06-22 07:18:32,654 ----------------------------------------------------------------------------------------------------
2025-06-22 07:20:50,709 epoch 3 - iter 2103/21032 - loss 0.83554641 - time (sec): 138.05 - samples/sec: 860.61 - lr: 0.000200 - momentum: 0.000000
2025-06-22 07:23:09,960 epoch 3 - iter 4206/21032 - loss 0.81350958 - time (sec): 277.31 - samples/sec: 859.19 - lr: 0.000200 - momentum: 0.000000
2025-06-22 07:25:28,828 epoch 3 - iter 6309/21032 - loss 0.79838913 - time (sec): 416.17 - samples/sec: 860.19 - lr: 0.000200 - momentum: 0.000000
2025-06-22 07:27:47,137 epoch 3 - iter 8412/21032 - loss 0.78282610 - time (sec): 554.48 - samples/sec: 860.34 - lr: 0.000200 - momentum: 0.000000
2025-06-22 07:30:05,091 epoch 3 - iter 10515/21032 - loss 0.77039624 - time (sec): 692.44 - samples/sec: 858.55 - lr: 0.000200 - momentum: 0.000000
2025-06-22 07:32:24,387 epoch 3 - iter 12618/21032 - loss 0.75754791 - time (sec): 831.73 - samples/sec: 857.71 - lr: 0.000200 - momentum: 0.000000
2025-06-22 07:34:42,388 epoch 3 - iter 14721/21032 - loss 0.74686437 - time (sec): 969.73 - samples/sec: 857.36 - lr: 0.000200 - momentum: 0.000000
2025-06-22 07:37:01,173 epoch 3 - iter 16824/21032 - loss 0.73394255 - time (sec): 1108.52 - samples/sec: 857.96 - lr: 0.000200 - momentum: 0.000000
2025-06-22 07:39:21,474 epoch 3 - iter 18927/21032 - loss 0.72228367 - time (sec): 1248.82 - samples/sec: 857.16 - lr: 0.000200 - momentum: 0.000000
2025-06-22 07:41:41,187 epoch 3 - iter 21030/21032 - loss 0.71177595 - time (sec): 1388.53 - samples/sec: 856.66 - lr: 0.000200 - momentum: 0.000000
2025-06-22 07:41:41,303 ----------------------------------------------------------------------------------------------------
2025-06-22 07:41:41,303 EPOCH 3 done: loss 0.7118 - lr: 0.000200
100% 329/329 [00:31<00:00, 10.40it/s]
2025-06-22 07:42:13,306 DEV : loss 0.7677895426750183 - f1-score (micro avg)  0.7733
2025-06-22 07:42:13,529  - 0 epochs without improvement
2025-06-22 07:42:13,529 saving best model
2025-06-22 07:42:14,371 ----------------------------------------------------------------------------------------------------
2025-06-22 07:44:32,398 epoch 4 - iter 2103/21032 - loss 0.59358602 - time (sec): 138.03 - samples/sec: 860.22 - lr: 0.000200 - momentum: 0.000000
2025-06-22 07:46:50,761 epoch 4 - iter 4206/21032 - loss 0.58625963 - time (sec): 276.39 - samples/sec: 862.16 - lr: 0.000200 - momentum: 0.000000
2025-06-22 07:49:08,687 epoch 4 - iter 6309/21032 - loss 0.57925446 - time (sec): 414.31 - samples/sec: 863.77 - lr: 0.000200 - momentum: 0.000000
2025-06-22 07:51:27,173 epoch 4 - iter 8412/21032 - loss 0.57376203 - time (sec): 552.80 - samples/sec: 864.45 - lr: 0.000200 - momentum: 0.000000
2025-06-22 07:53:44,491 epoch 4 - iter 10515/21032 - loss 0.56701633 - time (sec): 690.12 - samples/sec: 864.81 - lr: 0.000200 - momentum: 0.000000
2025-06-22 07:56:02,515 epoch 4 - iter 12618/21032 - loss 0.56092704 - time (sec): 828.14 - samples/sec: 864.16 - lr: 0.000200 - momentum: 0.000000
2025-06-22 07:58:20,047 epoch 4 - iter 14721/21032 - loss 0.55510589 - time (sec): 965.67 - samples/sec: 864.59 - lr: 0.000200 - momentum: 0.000000
2025-06-22 08:00:37,501 epoch 4 - iter 16824/21032 - loss 0.54903314 - time (sec): 1103.13 - samples/sec: 863.95 - lr: 0.000200 - momentum: 0.000000
2025-06-22 08:02:55,368 epoch 4 - iter 18927/21032 - loss 0.54473506 - time (sec): 1240.99 - samples/sec: 862.72 - lr: 0.000200 - momentum: 0.000000
2025-06-22 08:05:12,690 epoch 4 - iter 21030/21032 - loss 0.53991032 - time (sec): 1378.32 - samples/sec: 863.01 - lr: 0.000200 - momentum: 0.000000
2025-06-22 08:05:12,801 ----------------------------------------------------------------------------------------------------
2025-06-22 08:05:12,801 EPOCH 4 done: loss 0.5399 - lr: 0.000200
100% 329/329 [00:31<00:00, 10.46it/s]
2025-06-22 08:05:44,582 DEV : loss 0.7296582460403442 - f1-score (micro avg)  0.7813
2025-06-22 08:05:44,808  - 0 epochs without improvement
2025-06-22 08:05:44,808 saving best model
2025-06-22 08:05:45,621 ----------------------------------------------------------------------------------------------------
2025-06-22 08:08:01,828 epoch 5 - iter 2103/21032 - loss 0.48209727 - time (sec): 136.21 - samples/sec: 870.66 - lr: 0.000200 - momentum: 0.000000
2025-06-22 08:10:18,885 epoch 5 - iter 4206/21032 - loss 0.48047511 - time (sec): 273.26 - samples/sec: 867.51 - lr: 0.000200 - momentum: 0.000000
2025-06-22 08:12:36,197 epoch 5 - iter 6309/21032 - loss 0.47939693 - time (sec): 410.57 - samples/sec: 866.76 - lr: 0.000200 - momentum: 0.000000
2025-06-22 08:14:54,722 epoch 5 - iter 8412/21032 - loss 0.47576889 - time (sec): 549.10 - samples/sec: 863.62 - lr: 0.000200 - momentum: 0.000000
2025-06-22 08:17:12,460 epoch 5 - iter 10515/21032 - loss 0.47202718 - time (sec): 686.84 - samples/sec: 863.56 - lr: 0.000200 - momentum: 0.000000
2025-06-22 08:19:31,740 epoch 5 - iter 12618/21032 - loss 0.46946307 - time (sec): 826.12 - samples/sec: 862.85 - lr: 0.000200 - momentum: 0.000000
2025-06-22 08:21:49,331 epoch 5 - iter 14721/21032 - loss 0.46571471 - time (sec): 963.71 - samples/sec: 863.83 - lr: 0.000200 - momentum: 0.000000
2025-06-22 08:24:06,456 epoch 5 - iter 16824/21032 - loss 0.46353143 - time (sec): 1100.83 - samples/sec: 864.04 - lr: 0.000200 - momentum: 0.000000
2025-06-22 08:26:24,750 epoch 5 - iter 18927/21032 - loss 0.46039217 - time (sec): 1239.13 - samples/sec: 864.06 - lr: 0.000200 - momentum: 0.000000
2025-06-22 08:28:43,288 epoch 5 - iter 21030/21032 - loss 0.45769752 - time (sec): 1377.67 - samples/sec: 863.42 - lr: 0.000200 - momentum: 0.000000
2025-06-22 08:28:43,401 ----------------------------------------------------------------------------------------------------
2025-06-22 08:28:43,401 EPOCH 5 done: loss 0.4577 - lr: 0.000200
100% 329/329 [00:32<00:00, 10.22it/s]
2025-06-22 08:29:15,971 DEV : loss 0.7145301699638367 - f1-score (micro avg)  0.7991
2025-06-22 08:29:16,198  - 0 epochs without improvement
2025-06-22 08:29:16,198 saving best model
2025-06-22 08:29:17,047 ----------------------------------------------------------------------------------------------------
2025-06-22 08:31:35,823 epoch 6 - iter 2103/21032 - loss 0.41827704 - time (sec): 138.77 - samples/sec: 851.19 - lr: 0.000200 - momentum: 0.000000
2025-06-22 08:33:54,239 epoch 6 - iter 4206/21032 - loss 0.42066249 - time (sec): 277.19 - samples/sec: 856.28 - lr: 0.000200 - momentum: 0.000000
2025-06-22 08:36:11,991 epoch 6 - iter 6309/21032 - loss 0.42273515 - time (sec): 414.94 - samples/sec: 858.86 - lr: 0.000200 - momentum: 0.000000
2025-06-22 08:38:30,388 epoch 6 - iter 8412/21032 - loss 0.42062867 - time (sec): 553.34 - samples/sec: 859.03 - lr: 0.000200 - momentum: 0.000000
2025-06-22 08:40:48,737 epoch 6 - iter 10515/21032 - loss 0.41977267 - time (sec): 691.69 - samples/sec: 858.59 - lr: 0.000200 - momentum: 0.000000
2025-06-22 08:43:06,578 epoch 6 - iter 12618/21032 - loss 0.41930529 - time (sec): 829.53 - samples/sec: 859.04 - lr: 0.000200 - momentum: 0.000000
2025-06-22 08:45:23,679 epoch 6 - iter 14721/21032 - loss 0.41752648 - time (sec): 966.63 - samples/sec: 860.67 - lr: 0.000200 - momentum: 0.000000
2025-06-22 08:47:41,527 epoch 6 - iter 16824/21032 - loss 0.41558093 - time (sec): 1104.48 - samples/sec: 861.61 - lr: 0.000200 - momentum: 0.000000
2025-06-22 08:49:58,865 epoch 6 - iter 18927/21032 - loss 0.41426441 - time (sec): 1241.82 - samples/sec: 861.75 - lr: 0.000200 - momentum: 0.000000
2025-06-22 08:52:16,802 epoch 6 - iter 21030/21032 - loss 0.41184310 - time (sec): 1379.75 - samples/sec: 862.07 - lr: 0.000200 - momentum: 0.000000
2025-06-22 08:52:16,932 ----------------------------------------------------------------------------------------------------
2025-06-22 08:52:16,932 EPOCH 6 done: loss 0.4118 - lr: 0.000200
100% 329/329 [00:31<00:00, 10.42it/s]
2025-06-22 08:52:48,840 DEV : loss 0.7055701613426208 - f1-score (micro avg)  0.8037
2025-06-22 08:52:49,061  - 0 epochs without improvement
2025-06-22 08:52:49,061 saving best model
2025-06-22 08:52:49,886 ----------------------------------------------------------------------------------------------------
2025-06-22 08:55:05,282 epoch 7 - iter 2103/21032 - loss 0.39483125 - time (sec): 135.39 - samples/sec: 869.64 - lr: 0.000200 - momentum: 0.000000
2025-06-22 08:57:22,762 epoch 7 - iter 4206/21032 - loss 0.39319917 - time (sec): 272.87 - samples/sec: 865.90 - lr: 0.000200 - momentum: 0.000000
2025-06-22 08:59:41,807 epoch 7 - iter 6309/21032 - loss 0.39127789 - time (sec): 411.92 - samples/sec: 864.11 - lr: 0.000200 - momentum: 0.000000
2025-06-22 09:02:01,070 epoch 7 - iter 8412/21032 - loss 0.38890161 - time (sec): 551.18 - samples/sec: 863.05 - lr: 0.000200 - momentum: 0.000000
2025-06-22 09:04:19,223 epoch 7 - iter 10515/21032 - loss 0.38885290 - time (sec): 689.34 - samples/sec: 861.59 - lr: 0.000200 - momentum: 0.000000
2025-06-22 09:06:39,056 epoch 7 - iter 12618/21032 - loss 0.38734735 - time (sec): 829.17 - samples/sec: 860.41 - lr: 0.000200 - momentum: 0.000000
2025-06-22 09:08:58,238 epoch 7 - iter 14721/21032 - loss 0.38549056 - time (sec): 968.35 - samples/sec: 859.47 - lr: 0.000200 - momentum: 0.000000
2025-06-22 09:11:17,647 epoch 7 - iter 16824/21032 - loss 0.38426001 - time (sec): 1107.76 - samples/sec: 859.15 - lr: 0.000200 - momentum: 0.000000
2025-06-22 09:13:36,123 epoch 7 - iter 18927/21032 - loss 0.38259709 - time (sec): 1246.23 - samples/sec: 859.48 - lr: 0.000200 - momentum: 0.000000
2025-06-22 09:15:54,248 epoch 7 - iter 21030/21032 - loss 0.38121320 - time (sec): 1384.36 - samples/sec: 859.22 - lr: 0.000200 - momentum: 0.000000
2025-06-22 09:15:54,370 ----------------------------------------------------------------------------------------------------
2025-06-22 09:15:54,370 EPOCH 7 done: loss 0.3812 - lr: 0.000200
100% 329/329 [00:31<00:00, 10.35it/s]
2025-06-22 09:16:26,538 DEV : loss 0.6985462307929993 - f1-score (micro avg)  0.8054
2025-06-22 09:16:26,760  - 0 epochs without improvement
2025-06-22 09:16:26,760 saving best model
2025-06-22 09:16:27,581 ----------------------------------------------------------------------------------------------------
2025-06-22 09:18:45,507 epoch 8 - iter 2103/21032 - loss 0.36555471 - time (sec): 137.92 - samples/sec: 864.70 - lr: 0.000200 - momentum: 0.000000
2025-06-22 09:21:03,527 epoch 8 - iter 4206/21032 - loss 0.36691639 - time (sec): 275.94 - samples/sec: 865.02 - lr: 0.000200 - momentum: 0.000000
2025-06-22 09:23:22,647 epoch 8 - iter 6309/21032 - loss 0.36542110 - time (sec): 415.06 - samples/sec: 862.77 - lr: 0.000200 - momentum: 0.000000
2025-06-22 09:25:41,087 epoch 8 - iter 8412/21032 - loss 0.36426106 - time (sec): 553.50 - samples/sec: 861.63 - lr: 0.000200 - momentum: 0.000000
2025-06-22 09:28:00,237 epoch 8 - iter 10515/21032 - loss 0.36181559 - time (sec): 692.65 - samples/sec: 861.71 - lr: 0.000200 - momentum: 0.000000
2025-06-22 09:30:16,964 epoch 8 - iter 12618/21032 - loss 0.36036600 - time (sec): 829.38 - samples/sec: 861.38 - lr: 0.000200 - momentum: 0.000000
2025-06-22 09:32:34,867 epoch 8 - iter 14721/21032 - loss 0.35928216 - time (sec): 967.28 - samples/sec: 861.24 - lr: 0.000200 - momentum: 0.000000
2025-06-22 09:34:52,371 epoch 8 - iter 16824/21032 - loss 0.35852015 - time (sec): 1104.79 - samples/sec: 861.45 - lr: 0.000200 - momentum: 0.000000
2025-06-22 09:37:10,685 epoch 8 - iter 18927/21032 - loss 0.35818336 - time (sec): 1243.10 - samples/sec: 860.83 - lr: 0.000200 - momentum: 0.000000
2025-06-22 09:39:30,271 epoch 8 - iter 21030/21032 - loss 0.35716052 - time (sec): 1382.69 - samples/sec: 860.27 - lr: 0.000200 - momentum: 0.000000
2025-06-22 09:39:30,408 ----------------------------------------------------------------------------------------------------
2025-06-22 09:39:30,408 EPOCH 8 done: loss 0.3571 - lr: 0.000200
100% 329/329 [00:31<00:00, 10.36it/s]
2025-06-22 09:40:02,506 DEV : loss 0.6951723098754883 - f1-score (micro avg)  0.8064
2025-06-22 09:40:02,728  - 0 epochs without improvement
2025-06-22 09:40:02,729 saving best model
2025-06-22 09:40:03,566 ----------------------------------------------------------------------------------------------------
2025-06-22 09:42:20,330 epoch 9 - iter 2103/21032 - loss 0.34801333 - time (sec): 136.76 - samples/sec: 870.69 - lr: 0.000200 - momentum: 0.000000
2025-06-22 09:44:37,623 epoch 9 - iter 4206/21032 - loss 0.34712225 - time (sec): 274.06 - samples/sec: 868.77 - lr: 0.000200 - momentum: 0.000000
2025-06-22 09:46:55,817 epoch 9 - iter 6309/21032 - loss 0.34658603 - time (sec): 412.25 - samples/sec: 865.63 - lr: 0.000200 - momentum: 0.000000
2025-06-22 09:49:12,987 epoch 9 - iter 8412/21032 - loss 0.34526049 - time (sec): 549.42 - samples/sec: 867.40 - lr: 0.000200 - momentum: 0.000000
2025-06-22 09:51:30,479 epoch 9 - iter 10515/21032 - loss 0.34506510 - time (sec): 686.91 - samples/sec: 866.60 - lr: 0.000200 - momentum: 0.000000
2025-06-22 09:53:49,330 epoch 9 - iter 12618/21032 - loss 0.34284585 - time (sec): 825.76 - samples/sec: 864.73 - lr: 0.000200 - momentum: 0.000000
2025-06-22 09:56:09,600 epoch 9 - iter 14721/21032 - loss 0.34154854 - time (sec): 966.03 - samples/sec: 862.15 - lr: 0.000200 - momentum: 0.000000
2025-06-22 09:58:30,604 epoch 9 - iter 16824/21032 - loss 0.34058053 - time (sec): 1107.04 - samples/sec: 859.81 - lr: 0.000200 - momentum: 0.000000
2025-06-22 10:00:50,001 epoch 9 - iter 18927/21032 - loss 0.34086642 - time (sec): 1246.43 - samples/sec: 859.41 - lr: 0.000200 - momentum: 0.000000
2025-06-22 10:03:09,760 epoch 9 - iter 21030/21032 - loss 0.34020610 - time (sec): 1386.19 - samples/sec: 858.10 - lr: 0.000200 - momentum: 0.000000
2025-06-22 10:03:09,891 ----------------------------------------------------------------------------------------------------
2025-06-22 10:03:09,891 EPOCH 9 done: loss 0.3402 - lr: 0.000200
100% 329/329 [00:31<00:00, 10.36it/s]
2025-06-22 10:03:42,044 DEV : loss 0.6932265758514404 - f1-score (micro avg)  0.8065
2025-06-22 10:03:42,298  - 0 epochs without improvement
2025-06-22 10:03:42,298 saving best model
2025-06-22 10:03:43,161 ----------------------------------------------------------------------------------------------------
2025-06-22 10:06:01,352 epoch 10 - iter 2103/21032 - loss 0.33326581 - time (sec): 138.19 - samples/sec: 860.25 - lr: 0.000200 - momentum: 0.000000
2025-06-22 10:08:19,954 epoch 10 - iter 4206/21032 - loss 0.32873943 - time (sec): 276.79 - samples/sec: 860.25 - lr: 0.000200 - momentum: 0.000000
2025-06-22 10:10:40,266 epoch 10 - iter 6309/21032 - loss 0.32810203 - time (sec): 417.10 - samples/sec: 858.51 - lr: 0.000200 - momentum: 0.000000
2025-06-22 10:12:58,662 epoch 10 - iter 8412/21032 - loss 0.32729216 - time (sec): 555.50 - samples/sec: 858.15 - lr: 0.000200 - momentum: 0.000000
2025-06-22 10:15:17,461 epoch 10 - iter 10515/21032 - loss 0.32590187 - time (sec): 694.30 - samples/sec: 856.26 - lr: 0.000200 - momentum: 0.000000
2025-06-22 10:17:35,578 epoch 10 - iter 12618/21032 - loss 0.32593240 - time (sec): 832.42 - samples/sec: 856.25 - lr: 0.000200 - momentum: 0.000000
2025-06-22 10:19:54,313 epoch 10 - iter 14721/21032 - loss 0.32534411 - time (sec): 971.15 - samples/sec: 856.14 - lr: 0.000200 - momentum: 0.000000
2025-06-22 10:22:12,493 epoch 10 - iter 16824/21032 - loss 0.32495918 - time (sec): 1109.33 - samples/sec: 856.77 - lr: 0.000200 - momentum: 0.000000
2025-06-22 10:24:32,213 epoch 10 - iter 18927/21032 - loss 0.32357784 - time (sec): 1249.05 - samples/sec: 856.91 - lr: 0.000200 - momentum: 0.000000
2025-06-22 10:26:50,649 epoch 10 - iter 21030/21032 - loss 0.32323698 - time (sec): 1387.49 - samples/sec: 857.31 - lr: 0.000200 - momentum: 0.000000
2025-06-22 10:26:50,784 ----------------------------------------------------------------------------------------------------
2025-06-22 10:26:50,784 EPOCH 10 done: loss 0.3232 - lr: 0.000200
100% 329/329 [00:31<00:00, 10.38it/s]
2025-06-22 10:27:22,816 DEV : loss 0.6901296377182007 - f1-score (micro avg)  0.8069
2025-06-22 10:27:23,040  - 0 epochs without improvement
2025-06-22 10:27:23,041 saving best model
2025-06-22 10:27:24,536 ----------------------------------------------------------------------------------------------------
2025-06-22 10:27:24,537 Loading model from best epoch ...
2025-06-22 10:27:26,675 SequenceTagger predicts: Dictionary with 19 tags: O, S-LOCALIZACAO, B-LOCALIZACAO, E-LOCALIZACAO, I-LOCALIZACAO, S-NOME_EMPRESA, B-NOME_EMPRESA, E-NOME_EMPRESA, I-NOME_EMPRESA, S-SETOR, B-SETOR, E-SETOR, I-SETOR, S-PORTE, B-PORTE, E-PORTE, I-PORTE, <START>, <STOP>
100% 329/329 [01:03<00:00,  5.16it/s]
2025-06-22 10:28:30,895 
Results:
- F-score (micro) 0.9337
- F-score (macro) 0.9393
- Accuracy 0.8784

By class:
              precision    recall  f1-score   support

 LOCALIZACAO     0.9644    0.9414    0.9528     13711
NOME_EMPRESA     0.8202    0.8383    0.8291     13192
       SETOR     0.9926    0.9630    0.9776     11067
       PORTE     0.9971    0.9985    0.9978     10229

   micro avg     0.9371    0.9303    0.9337     48199
   macro avg     0.9436    0.9353    0.9393     48199
weighted avg     0.9383    0.9303    0.9342     48199

2025-06-22 10:28:30,895 ----------------------------------------------------------------------------------------------------
